{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_qNSzzyaCbD"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "jmjh290raIky"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0Qjg6vuaHNt"
   },
   "source": [
    "# Neural machine translation with attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOpGoE2T-YXS"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/text/tutorials/nmt_with_attention\">\n",
    "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
    "    View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/nmt_with_attention.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/text/blob/master/docs/tutorials/nmt_with_attention.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/text/docs/tutorials/nmt_with_attention.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xh8WNEwYA3BW"
   },
   "source": [
    "This tutorial demonstrates how to train a sequence-to-sequence (seq2seq) model for Spanish-to-English translation roughly based on [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025v5) (Luong et al., 2015). \n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img width=400 src=\"https://www.tensorflow.org/images/tutorials/transformer/RNN%2Battention-words-spa.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <th colspan=1>This tutorial: An encoder/decoder connected by attention.</th>\n",
    "<tr>\n",
    "</table>\n",
    "\n",
    "While this architecture is somewhat outdated, it is still a very useful project to work through to get a deeper understanding of sequence-to-sequence models and attention mechanisms (before going on to [Transformers](transformer.ipynb))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CiwtNgENbx2g"
   },
   "source": [
    "\n",
    "\n",
    "This example assumes some knowledge of TensorFlow fundamentals below the level of a Keras layer:\n",
    "  * [Working with tensors](https://www.tensorflow.org/guide/tensor) directly\n",
    "  * [Writing custom `keras.Model`s and `keras.layers`](https://www.tensorflow.org/guide/keras/custom_layers_and_models)\n",
    "\n",
    "After training the model in this notebook, you will be able to input a Spanish sentence, such as \"*Â¿todavia estan en casa?*\", and return the English translation: \"*are you still at home?*\"\n",
    "\n",
    "The resulting model is exportable as a `tf.saved_model`, so it can be used in other TensorFlow environments.\n",
    "\n",
    "The translation quality is reasonable for a toy example, but the generated attention plot is perhaps more interesting. This shows which parts of the input sentence has the model's attention while translating:\n",
    "\n",
    "<img src=\"https://tensorflow.org/images/spanish-english.png\" alt=\"spanish-english attention plot\">\n",
    "\n",
    "Note: This example takes approximately 10 minutes to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yAmSR1FaqKrl"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DGFTkuRvzWqc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-text in /Users/danielchan/miniconda3/lib/python3.11/site-packages (2.17.0rc0)\n",
      "Requirement already satisfied: tensorflow<2.17,>=2.16.1 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from tensorflow-text) (2.16.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (23.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (2.29.0)\n",
      "Requirement already satisfied: setuptools in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (67.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (4.7.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (3.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (0.37.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (1.25.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (0.38.4)\n",
      "Requirement already satisfied: rich in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (13.7.1)\n",
      "Requirement already satisfied: namex in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (0.0.8)\n",
      "Requirement already satisfied: optree in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (2023.7.22)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (2.16.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (0.1.2)\n",
      "Requirement already satisfied: einops in /Users/danielchan/miniconda3/lib/python3.11/site-packages (0.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"tensorflow-text\"\n",
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /Users/danielchan/miniconda3/lib/python3.11/site-packages (3.9.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from matplotlib) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.23 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from matplotlib) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/danielchan/miniconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tnxXKDjq3jEL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import typing\n",
    "from typing import Any, Tuple\n",
    "\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_yq8kvIqoqQ"
   },
   "source": [
    "This tutorial uses a lot of low level API's where it's easy to get shapes wrong. This class is used to check shapes throughout the tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "KqFqKi4fqN9X"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "class ShapeChecker():\n",
    "  def __init__(self):\n",
    "    # Keep a cache of every axis-name seen\n",
    "    self.shapes = {}\n",
    "\n",
    "  def __call__(self, tensor, names, broadcast=False):\n",
    "    if not tf.executing_eagerly():\n",
    "      return\n",
    "\n",
    "    parsed = einops.parse_shape(tensor, names)\n",
    "\n",
    "    for name, new_dim in parsed.items():\n",
    "      old_dim = self.shapes.get(name, None)\n",
    "      \n",
    "      if (broadcast and new_dim == 1):\n",
    "        continue\n",
    "\n",
    "      if old_dim is None:\n",
    "        # If the axis name is new, add its length to the cache.\n",
    "        self.shapes[name] = new_dim\n",
    "        continue\n",
    "\n",
    "      if new_dim != old_dim:\n",
    "        raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
    "                         f\"    found: {new_dim}\\n\"\n",
    "                         f\"    expected: {old_dim}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjUROhJfH3ML"
   },
   "source": [
    "## The data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puE_K74DIE9W"
   },
   "source": [
    "The tutorial uses a language dataset provided by [Anki](http://www.manythings.org/anki/). This dataset contains language translation pairs in the format:\n",
    "\n",
    "```\n",
    "May I borrow this book?\tÂ¿Puedo tomar prestado este libro?\n",
    "```\n",
    "\n",
    "They have a variety of languages available, but this example uses the English-Spanish dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfodePkj3jEa"
   },
   "source": [
    "### Download and prepare the dataset\n",
    "\n",
    "For convenience, a copy of this dataset is hosted on Google Cloud, but you can also download your own copy. After downloading the dataset, here are the steps you need to take to prepare the data:\n",
    "\n",
    "1. Add a *start* and *end* token to each sentence.\n",
    "2. Clean the sentences by removing special characters.\n",
    "3. Create a word index and reverse word index (dictionaries mapping from word â id and id â word).\n",
    "4. Pad each sentence to a maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "kRVATYOgJs1b"
   },
   "outputs": [],
   "source": [
    "# Download the file\n",
    "import pathlib\n",
    "\n",
    "local_path = 'furigana_training_data.txt'\n",
    "path_to_file = pathlib.Path(local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "OHn4Dct23jEm"
   },
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "  text = path.read_text(encoding='utf-8')\n",
    "\n",
    "  lines = text.splitlines()\n",
    "  pairs = [line.split('|') for line in lines]\n",
    "\n",
    "  context = np.array([context for target, context in pairs])\n",
    "  target = np.array([target for target, context in pairs])\n",
    "\n",
    "  return target, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ã§æ¥æ¬èªå¤§è¾å¸ããã³ã¢ã¹åããçµ±è¨çã°ãã¤ãã¨æ±é¦äºéæ ªå¼ä¼ç¤¾ãäºããä»ä¾ ã«çããã¾ã§ã¤ãã³ãä¼å ´ã­åäººæªè¸ããæ°éã«çæ¾é¡ãçç± ãå¾åããè¥¿å¤§åé³¥ã­çµã­å©ã¡ããå­ã®æ°å¦»ãåé¢å±æã­å¤ææ¸ãã²ãå½¢ãé©ç¸ãä»£ä¼ãçãåã¯ç¶ãç¶ããè´ä½ãé£¼ãèãããã·ã³ã°ã«ï¼©ï¼´ãããéè¡ã¸å°éç©ºç½åãåç·¨ä¸å¾ã­é¼ç¬éã§åã¿ã¡ããã¯æ®ããã\n"
     ]
    }
   ],
   "source": [
    "context_raw, target_raw = load_data(path_to_file)\n",
    "print(context_raw[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "lH_dPY8TRp3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ã§ã«ã»ããã ããã¦ãããã³ã¢ã¹ããããã¨ãããã¦ãã°ãã¤ãã¨ã¨ãã»ãããããã¶ãããããããããããã«ããããã«ã¾ããã¾ã§ã¤ãã³ããããããã­ããããã¿ã¨ããããã¯ãã«ã¾ãããã¿ãã¨ããããã¨ããããã«ãããã¡ã©ãã­ãããã­ã°ãã¡ãããã®ã«ãã¥ã¾ãããããã¦ãã¼ãã­ããããããã²ãããããããã©ããã ãããããããã©ã¯ãããããã©ããããããã°ãããã·ã³ã°ã«ã¢ã¤ãã£ã¼ãããããããã¸ãããããã¯ãããããããºããã¡ãã¤ã­ãã¦ãããã§ãã¿ã¡ããã¯ãããã°ãããã\n"
     ]
    }
   ],
   "source": [
    "print(target_raw[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgCLkfv5uO3d"
   },
   "source": [
    "### Create a tf.data dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PfVWx3WaI5Df"
   },
   "source": [
    "From these arrays of strings you can create a `tf.data.Dataset` of strings that shuffles and batches them efficiently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "3rZFgz69nMPa"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(context_raw)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "is_train = np.random.uniform(size=(len(target_raw),)) < 0.8\n",
    "\n",
    "train_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((context_raw[is_train], target_raw[is_train]))\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE))\n",
    "val_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((context_raw[~is_train], target_raw[~is_train]))\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ...,  True,  True,  True])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "qc6-NK1GtWQt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'\\xe3\\x81\\xab\\xe8\\xa1\\x97\\xe4\\xb8\\xa6\\xe3\\x81\\xad\\xe5\\xa4\\xaa\\xe9\\x99\\xbd\\xe9\\xb3\\xa5\\xe3\\x81\\xaa\\xe3\\x82\\xb3\\xe3\\x83\\x8f\\xe3\\x82\\xaf\\xe9\\x85\\xb8\\xe3\\x82\\x82\\xe8\\xbe\\xbb\\xe7\\x95\\xaa\\xe3\\x81\\xa8\\xe6\\x8a\\xb9\\xe8\\x8c\\xb6\\xe5\\xa1\\xa9\\xe3\\x81\\xa7\\xe4\\xb8\\x83\\xe3\\x80\\x87\\xe3\\x80\\x87\\xe3\\x80\\x87\\xe3\\x81\\xa8\\xe7\\xac\\x91\\xe3\\x81\\x84\\xe3\\x82\\xb8\\xe3\\x83\\xaf\\xe3\\x81\\x8c\\xe4\\xbd\\x95\\xe3\\x81\\xa8\\xe3\\x81\\xaa\\xe3\\x81\\x8f\\xe3\\x81\\xa7\\xe6\\x9d\\xb1\\xe5\\xb1\\xb1\\xe6\\x99\\x82\\xe4\\xbb\\xa3\\xe3\\x81\\xad\\xe5\\x8f\\x96\\xe3\\x82\\x8a\\xe8\\xaa\\xac\\xe3\\x81\\x8c\\xe4\\xbb\\x98\\xe4\\xba\\xba\\xe3\\x81\\xae\\xe7\\x89\\xb9\\xe5\\x88\\xa5\\xe7\\xa0\\x94\\xe7\\xa9\\xb6\\xe5\\x93\\xa1\\xe3\\x81\\xbe\\xe3\\x81\\xa7\\xe5\\xaf\\x9d\\xe4\\xbb\\x98\\xe3\\x81\\x8d\\xe3\\x81\\xae\\xe8\\x89\\xaf\\xe3\\x81\\x84\\xe3\\x82\\x88\\xe5\\xb2\\xa9\\xe6\\xb2\\xa2\\xe7\\x80\\x89\\xe3\\x81\\xaa\\xe4\\xb9\\x97\\xe3\\x82\\x8a\\xe7\\x9d\\x80\\xe3\\x81\\x91\\xe3\\x82\\x8b\\xe3\\x81\\xa8\\xe6\\xb0\\x97\\xe8\\xb1\\xa1\\xe3\\x83\\xac\\xe3\\x83\\xbc\\xe3\\x83\\x80\\xe3\\x83\\xbc\\xe3\\x81\\xaf\\xe3\\x81\\xaa\\xe3\\x81\\x9a\\xe3\\x82\\x89\\xe3\\x81\\x88\\xe6\\xad\\x8c\\xe3\\x81\\xad\\xe3\\x82\\x8f\\xe3\\x82\\x93\\xe3\\x82\\x8f\\xe3\\x82\\x93\\xe7\\x89\\xa9\\xe8\\xaa\\x9e\\xe3\\x82\\x84\\xe9\\xa4\\x85\\xe9\\xba\\xa6\\xe3\\x81\\xab\\xe9\\x9b\\xa8\\xe5\\xbe\\x8c\\xe3\\x81\\xae\\xe3\\x82\\xbf\\xe3\\x82\\xb1\\xe3\\x83\\x8e\\xe3\\x82\\xb3\\xe3\\x81\\xad\\xe5\\x85\\x88\\xe7\\x89\\xa9\\xe3\\x82\\xaa\\xe3\\x83\\x97\\xe3\\x82\\xb7\\xe3\\x83\\xa7\\xe3\\x83\\xb3\\xe3\\x81\\xaf\\xe5\\xad\\x98\\xe3\\x81\\x98\\xe4\\xb8\\x8a\\xe3\\x81\\x92\\xe3\\x82\\x8b\\xe3\\x81\\x8b\\xe3\\x82\\x89\\xe3\\x81\\xa8\\xe3\\x82\\x8a\\xe3\\x81\\xa4\\xe3\\x81\\x8f\\xe5\\xb3\\xb6\\xe3\\x81\\x8c\\xe3\\x81\\xaa\\xe3\\x81\\x84\\xe3\\x81\\xa7'\n",
      " b'\\xe3\\x81\\xa8\\xe5\\xa4\\x9a\\xe7\\x92\\xb0\\xe8\\x8a\\xb3\\xe9\\xa6\\x99\\xe6\\x97\\x8f\\xe7\\x82\\xad\\xe5\\x8c\\x96\\xe6\\xb0\\xb4\\xe7\\xb4\\xa0\\xe3\\x81\\xab\\xe6\\x9a\\x97\\xe6\\xae\\xba\\xe6\\x9c\\xaa\\xe9\\x81\\x82\\xe3\\x81\\x8c\\xe6\\xa1\\x81\\xe3\\x81\\x82\\xe3\\x81\\xb5\\xe3\\x82\\x8c\\xe3\\x81\\xa7\\xe5\\x85\\xa5\\xe5\\x8a\\x9b\\xe6\\x96\\x87\\xe3\\x81\\xab\\xe9\\x99\\xaa\\xe8\\x86\\xb3\\xe3\\x81\\xad\\xe5\\xbc\\xbe\\xe3\\x81\\x8d\\xe3\\x81\\xa0\\xe3\\x81\\x99\\xe3\\x81\\xaa\\xe3\\x81\\x8a\\xe3\\x82\\x8a\\xe8\\xbf\\x94\\xe3\\x81\\x99\\xe3\\x81\\xaa\\xe6\\x91\\xa9\\xe5\\xb0\\xbc\\xe7\\x8f\\xa0\\xe3\\x81\\xaa\\xe4\\xb8\\x91\\xe4\\xb8\\x89\\xe6\\x99\\x82\\xe3\\x81\\xaf\\xe9\\x9b\\xaa\\xe8\\xbe\\xb1\\xe3\\x82\\x88\\xe5\\x8b\\xbe\\xe9\\x85\\x8d\\xe3\\x81\\xa7\\xe5\\x89\\x8d\\xe6\\x96\\xb9\\xe5\\x86\\x8d\\xe8\\xa3\\x9c\\xe7\\xb5\\xa6\\xe7\\x82\\xb9\\xe3\\x81\\xa8\\xe7\\x9b\\xb4\\xe6\\xaf\\x98\\xe7\\xa5\\x9e\\xe3\\x82\\x84\\xe7\\xa9\\xba\\xe6\\x89\\x8b\\xe5\\xbd\\xa2\\xe3\\x81\\x8b\\xe3\\x82\\x89\\xe6\\xb5\\x85\\xe3\\x81\\x84\\xe3\\x81\\xaf\\xe8\\x90\\x8c\\xe3\\x81\\x88\\xe3\\x81\\xa0\\xe3\\x81\\x99\\xe3\\x81\\xab\\xe5\\x8f\\x97\\xe8\\x8b\\xa6\\xe5\\x9c\\x8f\\xe3\\x81\\xa8\\xe7\\x99\\xbd\\xe7\\xb4\\x99\\xe3\\x82\\x82\\xe3\\x81\\x8b\\xe3\\x81\\xa4\\xe3\\x81\\x8e\\xe5\\xb1\\x8b\\xe3\\x82\\x92'\n",
      " b'\\xe3\\x81\\xab\\xe6\\x89\\x93\\xe3\\x81\\xa4\\xe3\\x81\\x8b\\xe3\\x82\\x8a\\xe5\\x90\\x88\\xe3\\x81\\x86\\xe3\\x81\\x8b\\xe3\\x82\\x89\\xe6\\x98\\xa5\\xe9\\x9b\\xaa\\xe3\\x82\\x88\\xe7\\x8d\\xb2\\xe5\\xbe\\x97\\xe5\\x85\\x8d\\xe7\\x96\\xab\\xe3\\x82\\x84\\xe6\\xaf\\x94\\xe5\\x86\\x85\\xe5\\x9c\\xb0\\xe9\\xb6\\x8f\\xe3\\x81\\x8b\\xe3\\x82\\x89\\xe5\\x92\\x8c\\xe3\\x81\\x88\\xe3\\x82\\x8b\\xe3\\x81\\x8b\\xe9\\xb3\\xb6\\xe3\\x81\\xae\\xe8\\x80\\x85\\xe3\\x81\\xa8\\xe8\\xb7\\xb3\\xe6\\xba\\x80\\xe3\\x81\\xab\\xe6\\x9e\\x9d\\xe8\\xb1\\x86\\xe8\\xb1\\x86\\xe8\\x85\\x90\\xe3\\x81\\xaf\\xe3\\x83\\x9f\\xe3\\x82\\xaf\\xe3\\x82\\xbd\\xe3\\x82\\xbe\\xe3\\x82\\xa2\\xe9\\x96\\x80\\xe3\\x81\\xa7\\xe4\\xbb\\x8f\\xe5\\x9b\\xb3\\xe3\\x81\\xaf\\xe3\\x81\\xbb\\xe3\\x81\\xbb\\xe7\\xac\\x91\\xe3\\x82\\x80\\xe3\\x82\\x84\\xe5\\x86\\x92\\xe9\\xa0\\xad\\xe9\\x99\\xb3\\xe8\\xbf\\xb0\\xe3\\x81\\xab\\xe4\\xba\\x8b\\xe5\\x89\\x8d\\xe9\\x81\\x8b\\xe5\\x8b\\x95\\xe3\\x81\\xa7\\xe3\\x83\\x92\\xe3\\x83\\x9e\\xe3\\x83\\xa9\\xe3\\x83\\xa4\\xe9\\x9b\\xaa\\xe3\\x81\\xae\\xe4\\xb8\\x8b\\xe3\\x81\\x8c\\xe8\\x82\\x89\\xe8\\xa5\\xa6\\xe8\\xa2\\xa2\\xe3\\x81\\xad\\xe5\\x82\\xac\\xe3\\x81\\x97\\xe7\\x89\\xa9\\xe3\\x82\\x92\\xe5\\x87\\xba\\xe6\\x8f\\x83\\xe3\\x81\\x86\\xe3\\x81\\xb8\\xe7\\xa5\\x9e\\xe3\\x81\\xae\\xe5\\xbe\\xa1\\xe5\\xad\\x90\\xe3\\x81\\xa7\\xe9\\x81\\xa9\\xe5\\x9c\\xb0\\xe9\\x81\\xa9\\xe4\\xbd\\x9c\\xe3\\x81\\xad\\xe5\\x80\\x8b\\xe4\\xba\\xba\\xe5\\xba\\x83\\xe5\\x91\\x8a\\xe3\\x82\\x84\\xe5\\xa4\\xaa\\xe5\\x85\\x86\\xe3\\x81\\xaa'\n",
      " b'\\xe3\\x82\\x92\\xe7\\x8b\\xac\\xe9\\x80\\xb8\\xe3\\x81\\x8c\\xe4\\xbb\\xa3\\xe8\\xa8\\x80\\xe8\\x80\\x85\\xe3\\x81\\xbe\\xe3\\x81\\xa7\\xe5\\xae\\xa3\\xe4\\xbc\\x9d\\xe5\\xb7\\xa5\\xe4\\xbd\\x9c\\xe3\\x81\\xae\\xe7\\xad\\x86\\xe5\\xa1\\x9a\\xe3\\x81\\xbe\\xe3\\x81\\xa7\\xe6\\xb7\\xb7\\xe5\\x92\\x8c\\xe6\\x80\\xa7\\xe3\\x81\\x8b\\xe9\\x81\\xba\\xe5\\xb0\\xbf\\xe7\\x97\\x87\\xe3\\x81\\x8b\\xe5\\x9e\\x82\\xe3\\x82\\x8c\\xe6\\xa1\\x9c\\xe3\\x81\\xab\\xe4\\xb8\\x89\\xe8\\xa7\\x92\\xe7\\xb8\\x81\\xe7\\xa5\\x9e\\xe7\\x8d\\xa3\\xe9\\x8f\\xa1\\xe3\\x81\\xa7\\xe5\\xbc\\x97\\xe9\\x85\\xb8\\xe3\\x81\\xa8\\xe5\\xae\\x9a\\xe6\\x9c\\x9f\\xe9\\xa0\\x90\\xe9\\x87\\x91\\xe3\\x81\\x8b\\xe3\\x82\\x89\\xe4\\xb8\\x87\\xe4\\xba\\xba\\xe5\\x90\\x91\\xe3\\x81\\x91\\xe3\\x81\\x8b\\xef\\xbc\\xb4\\xef\\xbc\\xaf\\xef\\xbc\\xa2\\xe3\\x81\\xaa\\xe6\\x8b\\x85\\xe3\\x81\\x90\\xe3\\x81\\x8b\\xe5\\xb7\\x9e\\xe8\\xad\\xb0\\xe4\\xbc\\x9a\\xe3\\x81\\x8c\\xe9\\x80\\x80\\xe5\\xae\\x98\\xe3\\x81\\x8c\\xe7\\xa5\\x9e\\xe7\\x87\\x88\\xe3\\x81\\x8b\\xe9\\x96\\x91\\xe6\\x96\\x87\\xe5\\xad\\x97\\xe3\\x81\\xb8\\xe5\\xb0\\xb1\\xe8\\x81\\xb7\\xe6\\xb5\\xaa\\xe4\\xba\\xba\\xe3\\x81\\xad\\xe5\\x8f\\xb8\\xe4\\xbb\\xa4\\xe5\\xae\\x98\\xe3\\x82\\x92\\xe7\\x99\\xbd\\xe5\\x92\\x8c\\xe3\\x81\\x88\\xe3\\x81\\xa8\\xe3\\x81\\x8b\\xe3\\x81\\x8d\\xe5\\x87\\xba\\xe3\\x81\\x99\\xe3\\x81\\xaa\\xe8\\x83\\xb8\\xe7\\x86\\xb1\\xe3\\x81\\x8c\\xe6\\xb1\\xba\\xe8\\xb5\\xb7\\xe4\\xbc\\x9a\\xe3\\x81\\xa8\\xe5\\x85\\x8d\\xe7\\xa7\\x9f\\xe3\\x81\\xa8\\xe5\\x81\\x8f\\xe3\\x81\\xaf\\xe8\\x9b\\x99\\xe3\\x81\\x8b\\xe7\\x99\\xba\\xe6\\xb3\\xa1\\xe3\\x83\\x9d\\xe3\\x83\\xaa\\xe3\\x82\\xa6\\xe3\\x83\\xac\\xe3\\x82\\xbf\\xe3\\x83\\xb3\\xe3\\x81\\xaf\\xe7\\x8b\\x99\\xe3\\x81\\x84\\xe3\\x82\\x92\\xe3\\x81\\xa4\\xe3\\x81\\x91\\xe3\\x82\\x8b\\xe3\\x82\\x92\\xe4\\xb8\\x80\\xe9\\x81\\xba\\xe4\\xbc\\x9d\\xe5\\xad\\x90\\xe9\\x9b\\x91\\xe7\\xa8\\xae\\xe3\\x81\\xbe\\xe3\\x81\\xa7\\xe4\\xb8\\xa1\\xe6\\x89\\x8b\\xe5\\x88\\xa9\\xe3\\x81\\xab\\xef\\xbc\\xa2\\xef\\xbc\\xad\\xe3\\x81\\x8c\\xe8\\xa1\\x80\\xe7\\xae\\xa1\\xe9\\x81\\x8b\\xe5\\x8b\\x95\\xe6\\x80\\xa7\\xe9\\xbc\\xbb\\xe7\\x82\\x8e\\xe3\\x81\\x8b\\xe8\\xa5\\xbf\\xe5\\xb1\\xb1\\xe6\\xb4\\xbe\\xe3\\x81\\xa8\\xe4\\xbd\\x99\\xe7\\x8c\\xb6\\xe3\\x82\\x88'\n",
      " b'\\xe3\\x81\\xa8\\xe6\\xb5\\x81\\xe8\\xbb\\xa2\\xe7\\x94\\x9f\\xe6\\xad\\xbb\\xe3\\x81\\x8b\\xe3\\x82\\x89\\xe5\\xa5\\x87\\xe5\\xb2\\xa9\\xe6\\x80\\xaa\\xe7\\x9f\\xb3\\xe3\\x82\\x92\\xe5\\x9f\\xb7\\xe7\\x9d\\x80\\xe3\\x81\\x8c\\xe5\\x8d\\x98\\xe8\\xa1\\xa3\\xe3\\x81\\xbe\\xe3\\x81\\xa7\\xe6\\xac\\xa7\\xe5\\xb7\\x9e\\xe8\\xb5\\xa4\\xe6\\x9d\\xbe\\xe3\\x81\\xaf\\xe9\\x9a\\x9b\\xe7\\x89\\xa9\\xe3\\x81\\xad\\xe5\\x9c\\x9f\\xe3\\x81\\xae\\xe3\\x81\\x86\\xe3\\x81\\x8b\\xe3\\x82\\x89\\xe7\\xb4\\xb0\\xe3\\x83\\x9e\\xe3\\x83\\x83\\xe3\\x83\\x81\\xe3\\x83\\xa7\\xe3\\x81\\xae\\xe6\\x82\\xaa\\xe6\\xb3\\x95\\xe3\\x82\\x82\\xe5\\x8f\\x88\\xe6\\xb3\\x95\\xe3\\x81\\xaa\\xe3\\x82\\x8a\\xe3\\x81\\xbe\\xe3\\x81\\xa7\\xe9\\x80\\x80\\xe3\\x82\\x8b\\xe3\\x81\\xa8\\xe7\\xa0\\x82\\xe6\\xb4\\xb2\\xe3\\x81\\xad\\xe5\\xaf\\xba\\xe7\\xa4\\xbe\\xe4\\xbb\\x8f\\xe9\\x96\\xa3\\xe3\\x82\\x92\\xe6\\xa0\\xa1\\xe9\\x95\\xb7\\xe5\\x85\\x88\\xe7\\x94\\x9f\\xe3\\x81\\xbe\\xe3\\x81\\xa7\\xe5\\x87\\x9d\\xe3\\x81\\xa3\\xe3\\x81\\xa6\\xe3\\x81\\xaf\\xe6\\x80\\x9d\\xe6\\xa1\\x88\\xe3\\x81\\xab\\xe8\\x83\\xbd\\xe3\\x82\\x8f\\xe3\\x81\\x9a\\xe3\\x82\\x92\\xe5\\xbf\\xa0\\xe8\\xa8\\x80\\xe8\\x80\\xb3\\xe3\\x81\\xab\\xe9\\x80\\x86\\xe3\\x82\\x89\\xe3\\x81\\x86\\xe3\\x82\\x84\\xe3\\x83\\x9a\\xe3\\x82\\xa2\\xe3\\x82\\x92\\xe7\\xb5\\x84\\xe3\\x82\\x80\\xe3\\x81\\xbe\\xe3\\x81\\xa7\\xe9\\xb6\\xaf\\xe7\\xa5\\x9e\\xe6\\xa5\\xbd\\xe3\\x81\\x8c\\xe7\\x8d\\xa3\\xe5\\xae\\xb3\\xe3\\x81\\x8c\\xe3\\x81\\x9f\\xe3\\x81\\xa0\\xe9\\xa3\\x9f\\xe3\\x81\\x84\\xe3\\x81\\xa8\\xe6\\xb3\\xa8\\xe6\\x84\\x8f\\xe3\\x81\\xb8\\xe5\\xb2\\xa9\\xe6\\x89\\x8b\\xe7\\x9c\\x8c\\xe7\\xab\\x8b\\xe5\\xa4\\xa7\\xe5\\xad\\xa6\\xe3\\x81\\xab\\xe6\\xb0\\x97\\xe3\\x81\\x8c\\xe3\\x81\\xa4\\xe3\\x81\\x8f\\xe3\\x81\\x8b\\xe4\\xba\\xba\\xe9\\x9b\\x86\\xe3\\x82\\x81\\xe3\\x81\\xa8\\xe8\\xa1\\x80\\xe3\\x83\\x98\\xe3\\x83\\x89\\xe3\\x81\\xa8\\xe9\\x81\\x8e\\xe3\\x83\\xa8\\xe3\\x82\\xa6\\xe7\\xb4\\xa0\\xe9\\x85\\xb8\\xe3\\x81\\xb8\\xe3\\x82\\xa4\\xe3\\x83\\xa1\\xe3\\x83\\xbc\\xe3\\x82\\xb8\\xe7\\xae\\xa1\\xe3\\x81\\xae'], shape=(5,), dtype=string)\n",
      "\n",
      "tf.Tensor(\n",
      "[b'\\xe3\\x81\\xab\\xe3\\x81\\xbe\\xe3\\x81\\xa1\\xe3\\x81\\xaa\\xe3\\x81\\xbf\\xe3\\x81\\xad\\xe3\\x81\\x9f\\xe3\\x81\\x84\\xe3\\x82\\x88\\xe3\\x81\\x86\\xe3\\x81\\xa1\\xe3\\x82\\x87\\xe3\\x81\\x86\\xe3\\x81\\xaa\\xe3\\x81\\x93\\xe3\\x81\\xaf\\xe3\\x81\\x8f\\xe3\\x81\\x95\\xe3\\x82\\x93\\xe3\\x82\\x82\\xe3\\x81\\xa4\\xe3\\x81\\x98\\xe3\\x81\\xb0\\xe3\\x82\\x93\\xe3\\x81\\xa8\\xe3\\x81\\xbe\\xe3\\x81\\xa3\\xe3\\x81\\xa1\\xe3\\x82\\x83\\xe3\\x81\\x97\\xe3\\x81\\x8a\\xe3\\x81\\xa7\\xe3\\x81\\x97\\xe3\\x81\\xa1\\xe3\\x81\\x9b\\xe3\\x82\\x93\\xe3\\x81\\xa8\\xe3\\x82\\x8f\\xe3\\x82\\x89\\xe3\\x81\\x84\\xe3\\x81\\x98\\xe3\\x82\\x8f\\xe3\\x81\\x8c\\xe3\\x81\\xaa\\xe3\\x82\\x93\\xe3\\x81\\xa8\\xe3\\x81\\xaa\\xe3\\x81\\x8f\\xe3\\x81\\xa7\\xe3\\x81\\xb2\\xe3\\x81\\x8c\\xe3\\x81\\x97\\xe3\\x82\\x84\\xe3\\x81\\xbe\\xe3\\x81\\x98\\xe3\\x81\\xa0\\xe3\\x81\\x84\\xe3\\x81\\xad\\xe3\\x83\\x88\\xe3\\x83\\xaa\\xe3\\x82\\xbb\\xe3\\x83\\x84\\xe3\\x81\\x8c\\xe3\\x81\\xa4\\xe3\\x81\\x91\\xe3\\x81\\xb3\\xe3\\x81\\xa8\\xe3\\x81\\xae\\xe3\\x81\\xa8\\xe3\\x81\\x8f\\xe3\\x81\\xb9\\xe3\\x81\\xa4\\xe3\\x81\\x91\\xe3\\x82\\x93\\xe3\\x81\\x8d\\xe3\\x82\\x85\\xe3\\x81\\x86\\xe3\\x81\\x84\\xe3\\x82\\x93\\xe3\\x81\\xbe\\xe3\\x81\\xa7\\xe3\\x81\\xad\\xe3\\x81\\xa4\\xe3\\x81\\x8d\\xe3\\x81\\xae\\xe3\\x81\\x84\\xe3\\x81\\x84\\xe3\\x82\\x88\\xe3\\x82\\xa4\\xe3\\x83\\xaf\\xe3\\x82\\xaa\\xe3\\x83\\xa2\\xe3\\x83\\x80\\xe3\\x82\\xab\\xe3\\x81\\xaa\\xe3\\x81\\xae\\xe3\\x82\\x8a\\xe3\\x81\\xa4\\xe3\\x81\\x91\\xe3\\x82\\x8b\\xe3\\x81\\xa8\\xe3\\x81\\x8d\\xe3\\x81\\x97\\xe3\\x82\\x87\\xe3\\x81\\x86\\xe3\\x83\\xac\\xe3\\x83\\xbc\\xe3\\x83\\x80\\xe3\\x83\\xbc\\xe3\\x81\\xaf\\xe3\\x81\\xaa\\xe3\\x81\\x9a\\xe3\\x82\\x89\\xe3\\x81\\x88\\xe3\\x81\\x86\\xe3\\x81\\x9f\\xe3\\x81\\xad\\xe3\\x82\\x8f\\xe3\\x82\\x93\\xe3\\x82\\x8f\\xe3\\x82\\x93\\xe3\\x82\\x82\\xe3\\x81\\xae\\xe3\\x81\\x8c\\xe3\\x81\\x9f\\xe3\\x82\\x8a\\xe3\\x82\\x84\\xe3\\x82\\x82\\xe3\\x81\\xa1\\xe3\\x82\\x80\\xe3\\x81\\x8e\\xe3\\x81\\xab\\xe3\\x81\\x86\\xe3\\x81\\x94\\xe3\\x81\\xae\\xe3\\x81\\x9f\\xe3\\x81\\x91\\xe3\\x81\\xae\\xe3\\x81\\x93\\xe3\\x81\\xad\\xe3\\x81\\x95\\xe3\\x81\\x8d\\xe3\\x82\\x82\\xe3\\x81\\xae\\xe3\\x82\\xaa\\xe3\\x83\\x97\\xe3\\x82\\xb7\\xe3\\x83\\xa7\\xe3\\x83\\xb3\\xe3\\x81\\xaf\\xe3\\x81\\x9e\\xe3\\x82\\x93\\xe3\\x81\\x98\\xe3\\x81\\x82\\xe3\\x81\\x92\\xe3\\x82\\x8b\\xe3\\x81\\x8b\\xe3\\x82\\x89\\xe3\\x81\\xa8\\xe3\\x82\\x8a\\xe3\\x81\\xa4\\xe3\\x81\\x8f\\xe3\\x81\\x97\\xe3\\x81\\xbe\\xe3\\x81\\x8c\\xe3\\x81\\xaa\\xe3\\x81\\x84\\xe3\\x81\\xa7'\n",
      " b'\\xe3\\x81\\xa8\\xe3\\x81\\x9f\\xe3\\x81\\x8b\\xe3\\x82\\x93\\xe3\\x81\\xbb\\xe3\\x81\\x86\\xe3\\x81\\x93\\xe3\\x81\\x86\\xe3\\x81\\x9e\\xe3\\x81\\x8f\\xe3\\x81\\x9f\\xe3\\x82\\x93\\xe3\\x81\\x8b\\xe3\\x81\\x99\\xe3\\x81\\x84\\xe3\\x81\\x9d\\xe3\\x81\\xab\\xe3\\x81\\x82\\xe3\\x82\\x93\\xe3\\x81\\x95\\xe3\\x81\\xa4\\xe3\\x81\\xbf\\xe3\\x81\\x99\\xe3\\x81\\x84\\xe3\\x81\\x8c\\xe3\\x81\\x91\\xe3\\x81\\x9f\\xe3\\x81\\x82\\xe3\\x81\\xb5\\xe3\\x82\\x8c\\xe3\\x81\\xa7\\xe3\\x81\\xab\\xe3\\x82\\x85\\xe3\\x81\\x86\\xe3\\x82\\x8a\\xe3\\x82\\x87\\xe3\\x81\\x8f\\xe3\\x81\\xb6\\xe3\\x82\\x93\\xe3\\x81\\xab\\xe3\\x81\\xb0\\xe3\\x81\\x84\\xe3\\x81\\x9c\\xe3\\x82\\x93\\xe3\\x81\\xad\\xe3\\x81\\xaf\\xe3\\x81\\x98\\xe3\\x81\\x8d\\xe3\\x81\\xa0\\xe3\\x81\\x99\\xe3\\x81\\xaa\\xe3\\x81\\x8a\\xe3\\x82\\x8a\\xe3\\x81\\x8b\\xe3\\x81\\x88\\xe3\\x81\\x99\\xe3\\x81\\xaa\\xe3\\x81\\xbe\\xe3\\x81\\xab\\xe3\\x81\\x98\\xe3\\x82\\x85\\xe3\\x81\\xaa\\xe3\\x81\\x86\\xe3\\x81\\x97\\xe3\\x81\\xbf\\xe3\\x81\\xa4\\xe3\\x81\\xa9\\xe3\\x81\\x8d\\xe3\\x81\\xaf\\xe3\\x81\\x9b\\xe3\\x81\\xa4\\xe3\\x81\\x98\\xe3\\x82\\x87\\xe3\\x81\\x8f\\xe3\\x82\\x88\\xe3\\x81\\x93\\xe3\\x81\\x86\\xe3\\x81\\xb0\\xe3\\x81\\x84\\xe3\\x81\\xa7\\xe3\\x81\\x9c\\xe3\\x82\\x93\\xe3\\x81\\xbd\\xe3\\x81\\x86\\xe3\\x81\\x95\\xe3\\x81\\x84\\xe3\\x81\\xbb\\xe3\\x81\\x8d\\xe3\\x82\\x85\\xe3\\x81\\x86\\xe3\\x81\\xa6\\xe3\\x82\\x93\\xe3\\x81\\xa8\\xe3\\x81\\xaa\\xe3\\x81\\x8a\\xe3\\x81\\xb3\\xe3\\x81\\xae\\xe3\\x81\\x8b\\xe3\\x81\\xbf\\xe3\\x82\\x84\\xe3\\x81\\x8f\\xe3\\x81\\x86\\xe3\\x81\\xa6\\xe3\\x81\\x8c\\xe3\\x81\\x9f\\xe3\\x81\\x8b\\xe3\\x82\\x89\\xe3\\x81\\x82\\xe3\\x81\\x95\\xe3\\x81\\x84\\xe3\\x81\\xaf\\xe3\\x82\\x82\\xe3\\x81\\x88\\xe3\\x81\\xa0\\xe3\\x81\\x99\\xe3\\x81\\xab\\xe3\\x81\\x98\\xe3\\x82\\x85\\xe3\\x81\\x8f\\xe3\\x81\\x91\\xe3\\x82\\x93\\xe3\\x81\\xa8\\xe3\\x81\\x97\\xe3\\x82\\x89\\xe3\\x81\\x8b\\xe3\\x81\\xbf\\xe3\\x82\\x82\\xe3\\x81\\x8b\\xe3\\x81\\xa4\\xe3\\x81\\x8e\\xe3\\x82\\x84\\xe3\\x82\\x92'\n",
      " b'\\xe3\\x81\\xab\\xe3\\x81\\xb6\\xe3\\x81\\xa4\\xe3\\x81\\x8b\\xe3\\x82\\x8a\\xe3\\x81\\x82\\xe3\\x81\\x86\\xe3\\x81\\x8b\\xe3\\x82\\x89\\xe3\\x81\\x97\\xe3\\x82\\x85\\xe3\\x82\\x93\\xe3\\x81\\x9b\\xe3\\x81\\xa4\\xe3\\x82\\x88\\xe3\\x81\\x8b\\xe3\\x81\\x8f\\xe3\\x81\\xa8\\xe3\\x81\\x8f\\xe3\\x82\\x81\\xe3\\x82\\x93\\xe3\\x81\\x88\\xe3\\x81\\x8d\\xe3\\x82\\x84\\xe3\\x81\\xb2\\xe3\\x81\\xaa\\xe3\\x81\\x84\\xe3\\x81\\x98\\xe3\\x81\\xa9\\xe3\\x82\\x8a\\xe3\\x81\\x8b\\xe3\\x82\\x89\\xe3\\x81\\x82\\xe3\\x81\\x88\\xe3\\x82\\x8b\\xe3\\x81\\x8b\\xe3\\x81\\xa8\\xe3\\x81\\xb3\\xe3\\x81\\xae\\xe3\\x82\\x82\\xe3\\x81\\xae\\xe3\\x81\\xa8\\xe3\\x83\\x8f\\xe3\\x83\\x8d\\xe3\\x83\\x9e\\xe3\\x83\\xb3\\xe3\\x81\\xab\\xe3\\x81\\x88\\xe3\\x81\\xa0\\xe3\\x81\\xbe\\xe3\\x82\\x81\\xe3\\x81\\xa8\\xe3\\x81\\x86\\xe3\\x81\\xb5\\xe3\\x81\\xaf\\xe3\\x83\\x9f\\xe3\\x82\\xaf\\xe3\\x82\\xbd\\xe3\\x82\\xbe\\xe3\\x82\\xa2\\xe3\\x82\\x82\\xe3\\x82\\x93\\xe3\\x81\\xa7\\xe3\\x81\\xb5\\xe3\\x81\\xa8\\xe3\\x81\\xaf\\xe3\\x81\\xbb\\xe3\\x81\\x8a\\xe3\\x81\\x88\\xe3\\x82\\x80\\xe3\\x82\\x84\\xe3\\x81\\xbc\\xe3\\x81\\x86\\xe3\\x81\\xa8\\xe3\\x81\\x86\\xe3\\x81\\xa1\\xe3\\x82\\x93\\xe3\\x81\\x98\\xe3\\x82\\x85\\xe3\\x81\\xa4\\xe3\\x81\\xab\\xe3\\x81\\x98\\xe3\\x81\\x9c\\xe3\\x82\\x93\\xe3\\x81\\x86\\xe3\\x82\\x93\\xe3\\x81\\xa9\\xe3\\x81\\x86\\xe3\\x81\\xa7\\xe3\\x83\\x92\\xe3\\x83\\x9e\\xe3\\x83\\xa9\\xe3\\x83\\xa4\\xe3\\x82\\x86\\xe3\\x81\\x8d\\xe3\\x81\\xae\\xe3\\x81\\x97\\xe3\\x81\\x9f\\xe3\\x81\\x8c\\xe3\\x81\\xab\\xe3\\x81\\x8f\\xe3\\x82\\xb8\\xe3\\x83\\x90\\xe3\\x83\\xb3\\xe3\\x81\\xad\\xe3\\x82\\x82\\xe3\\x82\\x88\\xe3\\x81\\x86\\xe3\\x81\\x97\\xe3\\x82\\x82\\xe3\\x81\\xae\\xe3\\x82\\x92\\xe3\\x81\\xa7\\xe3\\x81\\x9d\\xe3\\x82\\x8d\\xe3\\x81\\x86\\xe3\\x81\\xb8\\xe3\\x81\\x8b\\xe3\\x81\\xbf\\xe3\\x81\\xae\\xe3\\x81\\xbf\\xe3\\x81\\x93\\xe3\\x81\\xa7\\xe3\\x81\\xa6\\xe3\\x81\\x8d\\xe3\\x81\\xa1\\xe3\\x81\\xa6\\xe3\\x81\\x8d\\xe3\\x81\\x95\\xe3\\x81\\x8f\\xe3\\x81\\xad\\xe3\\x81\\x93\\xe3\\x81\\x98\\xe3\\x82\\x93\\xe3\\x81\\x93\\xe3\\x81\\x86\\xe3\\x81\\x93\\xe3\\x81\\x8f\\xe3\\x82\\x84\\xe3\\x81\\xb5\\xe3\\x81\\xa8\\xe3\\x81\\xbe\\xe3\\x81\\xab\\xe3\\x81\\xaa'\n",
      " b'\\xe3\\x82\\x92\\xe3\\x83\\x89\\xe3\\x82\\xa4\\xe3\\x83\\x84\\xe3\\x81\\x8c\\xe3\\x81\\xa0\\xe3\\x81\\x84\\xe3\\x81\\x92\\xe3\\x82\\x93\\xe3\\x81\\x97\\xe3\\x82\\x83\\xe3\\x81\\xbe\\xe3\\x81\\xa7\\xe3\\x81\\x9b\\xe3\\x82\\x93\\xe3\\x81\\xa7\\xe3\\x82\\x93\\xe3\\x81\\x93\\xe3\\x81\\x86\\xe3\\x81\\x95\\xe3\\x81\\x8f\\xe3\\x81\\xae\\xe3\\x81\\xb5\\xe3\\x81\\xa7\\xe3\\x81\\xa5\\xe3\\x81\\x8b\\xe3\\x81\\xbe\\xe3\\x81\\xa7\\xe3\\x81\\x93\\xe3\\x82\\x93\\xe3\\x82\\x8f\\xe3\\x81\\x9b\\xe3\\x81\\x84\\xe3\\x81\\x8b\\xe3\\x81\\x84\\xe3\\x81\\xab\\xe3\\x82\\x87\\xe3\\x81\\x86\\xe3\\x81\\x97\\xe3\\x82\\x87\\xe3\\x81\\x86\\xe3\\x81\\x8b\\xe3\\x82\\xb7\\xe3\\x83\\x80\\xe3\\x83\\xac\\xe3\\x82\\xb6\\xe3\\x82\\xaf\\xe3\\x83\\xa9\\xe3\\x81\\xab\\xe3\\x81\\x95\\xe3\\x82\\x93\\xe3\\x81\\x8b\\xe3\\x81\\x8f\\xe3\\x81\\xb6\\xe3\\x81\\xa1\\xe3\\x81\\x97\\xe3\\x82\\x93\\xe3\\x81\\x98\\xe3\\x82\\x85\\xe3\\x81\\x86\\xe3\\x81\\x8d\\xe3\\x82\\x87\\xe3\\x81\\x86\\xe3\\x81\\xa7\\xe3\\x81\\xb5\\xe3\\x81\\xa3\\xe3\\x81\\x95\\xe3\\x82\\x93\\xe3\\x81\\xa8\\xe3\\x81\\xa6\\xe3\\x81\\x84\\xe3\\x81\\x8d\\xe3\\x82\\x88\\xe3\\x81\\x8d\\xe3\\x82\\x93\\xe3\\x81\\x8b\\xe3\\x82\\x89\\xe3\\x81\\xb0\\xe3\\x82\\x93\\xe3\\x81\\xab\\xe3\\x82\\x93\\xe3\\x82\\x80\\xe3\\x81\\x91\\xe3\\x81\\x8b\\xe3\\x83\\x86\\xe3\\x82\\xa3\\xe3\\x83\\xbc\\xe3\\x83\\xbb\\xe3\\x82\\xaa\\xe3\\x83\\xbc\\xe3\\x83\\xbb\\xe3\\x83\\x93\\xe3\\x83\\xbc\\xe3\\x81\\xaa\\xe3\\x81\\x8b\\xe3\\x81\\xa4\\xe3\\x81\\x90\\xe3\\x81\\x8b\\xe3\\x81\\x97\\xe3\\x82\\x85\\xe3\\x81\\x86\\xe3\\x81\\x8e\\xe3\\x81\\x8b\\xe3\\x81\\x84\\xe3\\x81\\x8c\\xe3\\x81\\x9f\\xe3\\x81\\x84\\xe3\\x81\\x8b\\xe3\\x82\\x93\\xe3\\x81\\x8c\\xe3\\x81\\x97\\xe3\\x82\\x93\\xe3\\x81\\xa8\\xe3\\x81\\x86\\xe3\\x81\\x8b\\xe3\\x81\\x8b\\xe3\\x82\\x93\\xe3\\x82\\x82\\xe3\\x81\\x98\\xe3\\x81\\xb8\\xe3\\x81\\x97\\xe3\\x82\\x85\\xe3\\x81\\x86\\xe3\\x81\\x97\\xe3\\x82\\x87\\xe3\\x81\\x8f\\xe3\\x82\\x8d\\xe3\\x81\\x86\\xe3\\x81\\xab\\xe3\\x82\\x93\\xe3\\x81\\xad\\xe3\\x81\\x97\\xe3\\x82\\x8c\\xe3\\x81\\x84\\xe3\\x81\\x8b\\xe3\\x82\\x93\\xe3\\x82\\x92\\xe3\\x81\\x97\\xe3\\x82\\x8d\\xe3\\x81\\x82\\xe3\\x81\\x88\\xe3\\x81\\xa8\\xe3\\x81\\x8b\\xe3\\x81\\x8d\\xe3\\x81\\xa0\\xe3\\x81\\x99\\xe3\\x81\\xaa\\xe3\\x82\\x80\\xe3\\x81\\xad\\xe3\\x81\\x82\\xe3\\x81\\xa4\\xe3\\x81\\x8c\\xe3\\x81\\x91\\xe3\\x81\\xa3\\xe3\\x81\\x8d\\xe3\\x81\\x8b\\xe3\\x81\\x84\\xe3\\x81\\xa8\\xe3\\x82\\x81\\xe3\\x82\\x93\\xe3\\x81\\x9d\\xe3\\x81\\xa8\\xe3\\x81\\x8b\\xe3\\x81\\x9f\\xe3\\x81\\xbb\\xe3\\x81\\xaf\\xe3\\x81\\x8b\\xe3\\x81\\x88\\xe3\\x82\\x8b\\xe3\\x81\\x8b\\xe3\\x81\\xaf\\xe3\\x81\\xa3\\xe3\\x81\\xbd\\xe3\\x81\\x86\\xe3\\x83\\x9d\\xe3\\x83\\xaa\\xe3\\x82\\xa6\\xe3\\x83\\xac\\xe3\\x82\\xbf\\xe3\\x83\\xb3\\xe3\\x81\\xaf\\xe3\\x81\\xad\\xe3\\x82\\x89\\xe3\\x81\\x84\\xe3\\x82\\x92\\xe3\\x81\\xa4\\xe3\\x81\\x91\\xe3\\x82\\x8b\\xe3\\x82\\x92\\xe3\\x81\\x84\\xe3\\x81\\xa1\\xe3\\x81\\x84\\xe3\\x81\\xa7\\xe3\\x82\\x93\\xe3\\x81\\x97\\xe3\\x81\\x96\\xe3\\x81\\xa3\\xe3\\x81\\x97\\xe3\\x82\\x85\\xe3\\x81\\xbe\\xe3\\x81\\xa7\\xe3\\x82\\x8a\\xe3\\x82\\x87\\xe3\\x81\\x86\\xe3\\x81\\xa6\\xe3\\x81\\x8d\\xe3\\x81\\x8d\\xe3\\x81\\xab\\xe3\\x83\\x93\\xe3\\x83\\xbc\\xe3\\x82\\xa8\\xe3\\x83\\xa0\\xe3\\x81\\x8c\\xe3\\x81\\x91\\xe3\\x81\\xa3\\xe3\\x81\\x8b\\xe3\\x82\\x93\\xe3\\x81\\x86\\xe3\\x82\\x93\\xe3\\x81\\xa9\\xe3\\x81\\x86\\xe3\\x81\\x9b\\xe3\\x81\\x84\\xe3\\x81\\xb3\\xe3\\x81\\x88\\xe3\\x82\\x93\\xe3\\x81\\x8b\\xe3\\x81\\x9b\\xe3\\x81\\x84\\xe3\\x81\\x96\\xe3\\x82\\x93\\xe3\\x81\\xaf\\xe3\\x81\\xa8\\xe3\\x82\\x88\\xe3\\x82\\x86\\xe3\\x81\\x86\\xe3\\x82\\x88'\n",
      " b'\\xe3\\x81\\xa8\\xe3\\x82\\x8b\\xe3\\x81\\xa6\\xe3\\x82\\x93\\xe3\\x81\\x97\\xe3\\x82\\x87\\xe3\\x81\\x86\\xe3\\x81\\x98\\xe3\\x81\\x8b\\xe3\\x82\\x89\\xe3\\x81\\x8d\\xe3\\x81\\x8c\\xe3\\x82\\x93\\xe3\\x81\\x8b\\xe3\\x81\\x84\\xe3\\x81\\x9b\\xe3\\x81\\x8d\\xe3\\x82\\x92\\xe3\\x81\\x97\\xe3\\x82\\x85\\xe3\\x81\\x86\\xe3\\x81\\xa1\\xe3\\x82\\x83\\xe3\\x81\\x8f\\xe3\\x81\\x8c\\xe3\\x81\\x9f\\xe3\\x82\\x93\\xe3\\x81\\x84\\xe3\\x81\\xbe\\xe3\\x81\\xa7\\xe3\\x81\\x8a\\xe3\\x81\\x86\\xe3\\x81\\x97\\xe3\\x82\\x85\\xe3\\x81\\x86\\xe3\\x81\\x82\\xe3\\x81\\x8b\\xe3\\x81\\xbe\\xe3\\x81\\xa4\\xe3\\x81\\xaf\\xe3\\x81\\x8d\\xe3\\x82\\x8f\\xe3\\x82\\x82\\xe3\\x81\\xae\\xe3\\x81\\xad\\xe3\\x81\\xa9\\xe3\\x81\\xae\\xe3\\x81\\x86\\xe3\\x81\\x8b\\xe3\\x82\\x89\\xe3\\x81\\xbb\\xe3\\x81\\x9d\\xe3\\x83\\x9e\\xe3\\x83\\x83\\xe3\\x83\\x81\\xe3\\x83\\xa7\\xe3\\x81\\xae\\xe3\\x81\\x82\\xe3\\x81\\x8f\\xe3\\x81\\xbb\\xe3\\x81\\x86\\xe3\\x82\\x82\\xe3\\x81\\xbe\\xe3\\x81\\x9f\\xe3\\x81\\xbb\\xe3\\x81\\x86\\xe3\\x81\\xaa\\xe3\\x82\\x8a\\xe3\\x81\\xbe\\xe3\\x81\\xa7\\xe3\\x81\\x97\\xe3\\x81\\x95\\xe3\\x82\\x8b\\xe3\\x81\\xa8\\xe3\\x81\\x95\\xe3\\x81\\x99\\xe3\\x81\\xad\\xe3\\x81\\x98\\xe3\\x81\\x97\\xe3\\x82\\x83\\xe3\\x81\\xb6\\xe3\\x81\\xa3\\xe3\\x81\\x8b\\xe3\\x81\\x8f\\xe3\\x82\\x92\\xe3\\x81\\x93\\xe3\\x81\\x86\\xe3\\x81\\xa1\\xe3\\x82\\x87\\xe3\\x81\\x86\\xe3\\x81\\x9b\\xe3\\x82\\x93\\xe3\\x81\\x9b\\xe3\\x81\\x84\\xe3\\x81\\xbe\\xe3\\x81\\xa7\\xe3\\x81\\x93\\xe3\\x81\\xa3\\xe3\\x81\\xa6\\xe3\\x81\\xaf\\xe3\\x81\\x97\\xe3\\x81\\x82\\xe3\\x82\\x93\\xe3\\x81\\xab\\xe3\\x81\\x82\\xe3\\x81\\x9f\\xe3\\x82\\x8f\\xe3\\x81\\x9a\\xe3\\x82\\x92\\xe3\\x81\\xa1\\xe3\\x82\\x85\\xe3\\x81\\x86\\xe3\\x81\\x92\\xe3\\x82\\x93\\xe3\\x81\\xbf\\xe3\\x81\\xbf\\xe3\\x81\\xab\\xe3\\x81\\x95\\xe3\\x81\\x8b\\xe3\\x82\\x89\\xe3\\x81\\x86\\xe3\\x82\\x84\\xe3\\x83\\x9a\\xe3\\x82\\xa2\\xe3\\x82\\x92\\xe3\\x81\\x8f\\xe3\\x82\\x80\\xe3\\x81\\xbe\\xe3\\x81\\xa7\\xe3\\x82\\xa6\\xe3\\x82\\xb0\\xe3\\x82\\xa4\\xe3\\x82\\xb9\\xe3\\x82\\xab\\xe3\\x82\\xb0\\xe3\\x83\\xa9\\xe3\\x81\\x8c\\xe3\\x81\\x98\\xe3\\x82\\x85\\xe3\\x81\\x86\\xe3\\x81\\x8c\\xe3\\x81\\x84\\xe3\\x81\\x8c\\xe3\\x82\\xbf\\xe3\\x83\\x80\\xe3\\x81\\x90\\xe3\\x81\\x84\\xe3\\x81\\xa8\\xe3\\x81\\xa1\\xe3\\x82\\x85\\xe3\\x81\\x86\\xe3\\x81\\x84\\xe3\\x81\\xb8\\xe3\\x81\\x84\\xe3\\x82\\x8f\\xe3\\x81\\xa6\\xe3\\x81\\x91\\xe3\\x82\\x93\\xe3\\x82\\x8a\\xe3\\x81\\xa4\\xe3\\x81\\xa0\\xe3\\x81\\x84\\xe3\\x81\\x8c\\xe3\\x81\\x8f\\xe3\\x81\\xab\\xe3\\x81\\x8d\\xe3\\x81\\x8c\\xe3\\x81\\xa4\\xe3\\x81\\x8f\\xe3\\x81\\x8b\\xe3\\x81\\xb2\\xe3\\x81\\xa8\\xe3\\x81\\x82\\xe3\\x81\\xa4\\xe3\\x82\\x81\\xe3\\x81\\xa8\\xe3\\x81\\xa1\\xe3\\x81\\xb8\\xe3\\x81\\xa9\\xe3\\x81\\xa8\\xe3\\x81\\x8b\\xe3\\x83\\xa8\\xe3\\x82\\xa6\\xe3\\x81\\x9d\\xe3\\x81\\x95\\xe3\\x82\\x93\\xe3\\x81\\xb8\\xe3\\x82\\xa4\\xe3\\x83\\xa1\\xe3\\x83\\xbc\\xe3\\x82\\xb8\\xe3\\x81\\x8b\\xe3\\x82\\x93\\xe3\\x81\\xae'], shape=(5,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for example_context_strings, example_target_strings in train_raw.take(1):\n",
    "  print(example_context_strings[:5])\n",
    "  print()\n",
    "  print(example_target_strings[:5])\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCoxLcuN3bwv"
   },
   "source": [
    "### Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kwdPcHvzz_a"
   },
   "source": [
    "One of the goals of this tutorial is to build a model that can be exported as a `tf.saved_model`. To make that exported model useful it should take `tf.string` inputs, and return `tf.string` outputs: All the text processing happens inside the model. Mainly using a `layers.TextVectorization` layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOQ5n55X4uDB"
   },
   "source": [
    "#### Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upKhKAMK4zzI"
   },
   "source": [
    "The model is dealing with multilingual text with a limited vocabulary. So it will be important to standardize the input text.\n",
    "\n",
    "The first step is Unicode normalization to split accented characters and replace compatibility characters with their ASCII equivalents.\n",
    "\n",
    "The `tensorflow_text` package contains a unicode normalize operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "mD0e-DWGQ2Vo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xef\\xbc\\x93\\xe3\\x83\\xb6\\xe5\\x9b\\xbd\\xe8\\xaa\\x9e\\xe3\\x81\\x8c\\xe3\\x81\\xa7\\xe3\\x81\\x8d\\xe3\\x82\\x8b\\xe3\\x81\\x91\\xe3\\x81\\xa9\\xe3\\x80\\x81\\xe8\\x8b\\xb1\\xe8\\xaa\\x9e\\xe3\\x81\\xaf\\xe9\\x9b\\xa3\\xe3\\x81\\x97\\xe3\\x81\\x84\\xe3\\x81\\xa7\\xe3\\x81\\x99'\n",
      "b'3\\xe3\\x83\\xb6\\xe5\\x9b\\xbd\\xe8\\xaa\\x9e\\xe3\\x81\\x8c\\xe3\\x81\\xa7\\xe3\\x81\\x8d\\xe3\\x82\\x8b\\xe3\\x81\\x91\\xe3\\x81\\xa9\\xe3\\x80\\x81\\xe8\\x8b\\xb1\\xe8\\xaa\\x9e\\xe3\\x81\\xaf\\xe9\\x9b\\xa3\\xe3\\x81\\x97\\xe3\\x81\\x84\\xe3\\x81\\xa7\\xe3\\x81\\x99'\n",
      "3ã¶å½èªãã§ãããã©ãè±èªã¯é£ããã§ã\n"
     ]
    }
   ],
   "source": [
    "example_text = tf.constant('ï¼ã¶å½èªãã§ãããã©ãè±èªã¯é£ããã§ã')\n",
    "\n",
    "print(example_text.numpy())\n",
    "print(tf_text.normalize_utf8(example_text, 'NFKC').numpy())\n",
    "print(tf_text.normalize_utf8(example_text, 'NFKC').numpy().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hTllEjK6RSo"
   },
   "source": [
    "Unicode normalization will be the first step in the text standardization function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def containsJapanese(text):\n",
    "    re_kanji = re.compile(r'[\\u3000-\\u303f\\u3040-\\u309f\\u30a0-\\u30ff\\uff00-\\uff9f\\u4e00-\\u9faf\\u3400-\\u4dbf]')\n",
    "    return re_kanji.search(text) is not None;\n",
    "\n",
    "def containsKanji(text):\n",
    "    re_kanji = re.compile(r'[\\u4e00-\\u9faf\\u3400-\\u4dbf]')\n",
    "    return re_kanji.search(text) is not None;\n",
    "\n",
    "def containsHiragana(text):\n",
    "    re_hiragana = re.compile(r'[\\u3040-\\u309f]')\n",
    "    return re_hiragana.search(text) is not None;\n",
    "\n",
    "def get_state(text):\n",
    "    if containsKanji(text):\n",
    "        return 'kanji'\n",
    "    elif containsHiragana(text):\n",
    "        return 'hiragana'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "def insert_spaces(text):\n",
    "    result = \"\"\n",
    "    state = get_state(text[0])\n",
    "    i = 0\n",
    "    length = len(text)\n",
    "    while i < length:\n",
    "        if state != get_state(text[i]):\n",
    "            state = get_state(text[i])\n",
    "            result += ' '\n",
    "        result += text[i]\n",
    "        i += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "chTF5N885F0P"
   },
   "outputs": [],
   "source": [
    "def tf_lower_and_split_punct(text):\n",
    "    text = tf.strings.lower(text)\n",
    "    text = tf_text.normalize_utf8(text, 'NFKC')\n",
    "    # Punctuation removed, add later if needed\n",
    "    text = tf.strings.regex_replace(text, '[^\\u3000-\\u303f\\u3040-\\u309f\\u30a0-\\u30ff\\uff00-\\uff9f\\u4e00-\\u9faf\\u3400-\\u4dbf]', '')\n",
    "    # # Add spaces around punctuation.\n",
    "    text = tf.strings.regex_replace(text, '[\\u3000-\\u303f\\u3040-\\u309f\\u30a0-\\u30ff\\uff00-\\uff9f\\u4e00-\\u9faf\\u3400-\\u4dbf]', r' \\0 ')\n",
    "    # text = tf.strings.regex_replace(text, '[.?!,Â¿]', r' \\0 ')\n",
    "    # Strip whitespace.\n",
    "    text = tf.strings.strip(text)\n",
    "    \n",
    "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "UREvDg3sEKYa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï¼ã¶å½èªãã§ãããã©ãè±èªã¯é£ããã§ã\n",
      "[START] ã¶  å½  èª  ã  ã§  ã  ã  ã  ã©  ã  è±  èª  ã¯  é£  ã  ã  ã§  ã [END]\n"
     ]
    }
   ],
   "source": [
    "print(example_text.numpy().decode())\n",
    "print(tf_lower_and_split_punct(example_text).numpy().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4q-sKsSI7xRZ"
   },
   "source": [
    "#### Text Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6aKn8qd37abi"
   },
   "source": [
    "This standardization function will be wrapped up in a `tf.keras.layers.TextVectorization` layer which will handle the vocabulary extraction and conversion of input text to sequences of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "eAY9k49G3jE_"
   },
   "outputs": [],
   "source": [
    "max_vocab_size = 25000\n",
    "\n",
    "context_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size,\n",
    "    ragged=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kbC6ODP8IK_"
   },
   "source": [
    "The `TextVectorization` layer and many other [Keras preprocessing layers](https://www.tensorflow.org/guide/keras/preprocessing_layers) have an `adapt` method. This method reads one epoch of the training data, and works a lot like `Model.fit`. This `adapt` method initializes the layer based on the data. Here it determines the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "bmsI1Yql8FYe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-30 21:02:54.303787: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'ã', 'ã§', 'ã®', 'ã', 'ã', 'ã«', 'ãª', 'ã']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_text_processor.adapt(train_raw.map(lambda context, target: context))\n",
    "\n",
    "# Here are the first 10 words from the vocabulary:\n",
    "context_text_processor.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kGjIFjX8_Wp"
   },
   "source": [
    "That's the Spanish `TextVectorization` layer, now build and `.adapt()` the English one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "jlC4xuZnKLBS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-30 21:03:23.819347: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'ã', 'ã', 'ã', 'ã', 'ã', 'ã', 'ã', 'ã']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size,\n",
    "    ragged=True)\n",
    "\n",
    "target_text_processor.adapt(train_raw.map(lambda context, target: target))\n",
    "target_text_processor.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWQqlP_s9eIv"
   },
   "source": [
    "Now these layers can convert a batch of strings into a batch of token IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "9KZxj8IrNZ9S"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[21, 7, 1294, 868, 16, 349, 859, 261, 8, 114, 435, 62, 207, 10, 2056, 396,\n",
       "  11, 2596, 327, 433, 3, 646, 1197, 1197, 1197, 11, 650, 19, 143, 504, 6,\n",
       "  273, 11, 8, 28, 3, 305, 135, 119, 157, 16, 90, 20, 492, 6, 82, 34, 4, 403,\n",
       "  314, 438, 545, 359, 12, 3, 604, 82, 24, 4, 221, 19, 14, 619, 1289, 2524,\n",
       "  8, 393, 20, 308, 29, 18, 11, 63, 559, 115, 25, 318, 25, 13, 8, 128, 9, 50,\n",
       "  537, 16, 86, 60, 86, 60, 42, 103, 15, 993, 823, 7, 487, 239, 4, 72, 352,\n",
       "  397, 114, 16, 244, 42, 227, 149, 100, 505, 26, 13, 836, 162, 64, 98, 18,\n",
       "  2, 9, 11, 20, 53, 28, 548, 6, 8, 19, 3, 22]                               ,\n",
       " [21, 11, 284, 575, 1935, 649, 502, 769, 78, 76, 212, 7, 760, 542, 717,\n",
       "  2298, 6, 1995, 117, 709, 30, 3, 96, 120, 92, 7, 2486, 1984, 16, 618, 24,\n",
       "  225, 27, 8, 77, 20, 459, 27, 8, 1075, 1845, 1465, 8, 2466, 122, 119, 13,\n",
       "  429, 2476, 14, 2547, 441, 3, 183, 145, 518, 715, 857, 288, 11, 283, 2825,\n",
       "  116, 15, 176, 57, 154, 2, 9, 1638, 19, 13, 2135, 50, 225, 27, 7, 388, 653,\n",
       "  1457, 11, 139, 280, 10, 2, 53, 210, 181, 5, 22]                           ,\n",
       " [21, 7, 258, 53, 2, 20, 43, 35, 2, 9, 720, 429, 14, 2276, 512, 932, 1431,\n",
       "  15, 647, 160, 73, 938, 2, 9, 201, 50, 18, 2, 1759, 4, 52, 11, 1122, 752,\n",
       "  7, 1195, 513, 513, 1022, 13, 279, 62, 491, 2028, 58, 415, 3, 556, 454, 13,\n",
       "  665, 665, 650, 95, 15, 2215, 190, 1973, 1274, 7, 87, 183, 358, 88, 3, 700,\n",
       "  125, 54, 563, 429, 4, 97, 6, 344, 3173, 3091, 16, 1828, 23, 42, 5, 59,\n",
       "  1576, 35, 17, 116, 4, 94, 33, 3, 953, 73, 953, 179, 16, 994, 34, 632, 654,\n",
       "  15, 349, 2746, 8, 22]                                                     ]>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tokens = context_text_processor(example_context_strings)\n",
    "example_tokens[:3, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AA9rUn9G9n78"
   },
   "source": [
    "The `get_vocabulary` method can be used to convert token IDs back to text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128,), dtype=int64, numpy=\n",
       "array([  21,    7, 1294,  868,   16,  349,  859,  261,    8,  114,  435,\n",
       "         62,  207,   10, 2056,  396,   11, 2596,  327,  433,    3,  646,\n",
       "       1197, 1197, 1197,   11,  650,   19,  143,  504,    6,  273,   11,\n",
       "          8,   28,    3,  305,  135,  119,  157,   16,   90,   20,  492,\n",
       "          6,   82,   34,    4,  403,  314,  438,  545,  359,   12,    3,\n",
       "        604,   82,   24,    4,  221,   19,   14,  619, 1289, 2524,    8,\n",
       "        393,   20,  308,   29,   18,   11,   63,  559,  115,   25,  318,\n",
       "         25,   13,    8,  128,    9,   50,  537,   16,   86,   60,   86,\n",
       "         60,   42,  103,   15,  993,  823,    7,  487,  239,    4,   72,\n",
       "        352,  397,  114,   16,  244,   42,  227,  149,  100,  505,   26,\n",
       "         13,  836,  162,   64,   98,   18,    2,    9,   11,   20,   53,\n",
       "         28,  548,    6,    8,   19,    3,   22])>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "98g9rcxGQY0I"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[START] ã« è¡ ä¸¦ ã­ å¤ª é½ é³¥ ãª ã³ ã ã¯ é¸ ã è¾» çª ã¨ æ¹ è¶ å¡© ã§ ä¸ ã ã ã ã¨ ç¬ ã ã¸ ã¯ ã ä½ ã¨ ãª ã ã§ æ± å±± æ ä»£ ã­ å ã èª¬ ã ä» äºº ã® ç¹ å¥ ç  ç©¶ å¡ ã¾ ã§ å¯ ä» ã ã® è¯ ã ã å²© æ²¢ ç ãª ä¹ ã ç ã ã ã¨ æ° è±¡ ã¬ ã¼ ã ã¼ ã¯ ãª ã ã ã æ­ ã­ ã ã ã ã ç© èª ã é¤ éº¦ ã« é¨ å¾ ã® ã¿ ã± ã ã³ ã­ å ç© ãª ã ã· ã§ ã³ ã¯ å­ ã ä¸ ã ã ã ã ã¨ ã ã¤ ã å³¶ ã ãª ã ã§ [END]'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vocab = np.array(context_text_processor.get_vocabulary())\n",
    "tokens = context_vocab[example_tokens[0].numpy()]\n",
    "' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ot0aCL9t-Ghi"
   },
   "source": [
    "The returned token IDs are zero-padded. This can easily be turned into a mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "_jx4Or_eFRSz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Mask')"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.pcolormesh(example_tokens.to_tensor())\n",
    "plt.title('Token IDs')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pcolormesh(example_tokens.to_tensor() != 0)\n",
    "plt.title('Mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3O0B4XdFlRgc"
   },
   "source": [
    "### Process the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVCuyuSp_whd"
   },
   "source": [
    "The `process_text` function below converts the `Datasets` of strings, into  0-padded tensors of token IDs. It also converts from a `(context, target)` pair to an `((context, target_in), target_out)` pair for training with `keras.Model.fit`. Keras expects `(inputs, labels)` pairs, the inputs are the `(context, target_in)` and the labels are `target_out`. The difference between `target_in` and `target_out` is that they are shifted by one step relative to eachother, so that at each location the label is the next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "wk5tbZWQl5u1"
   },
   "outputs": [],
   "source": [
    "def process_text(context, target):\n",
    "  context = context_text_processor(context).to_tensor()\n",
    "  target = target_text_processor(target)\n",
    "  targ_in = target[:,:-1].to_tensor()\n",
    "  targ_out = target[:,1:].to_tensor()\n",
    "  return (context, targ_in), targ_out\n",
    "\n",
    "\n",
    "train_ds = train_raw.map(process_text, tf.data.AUTOTUNE)\n",
    "val_ds = val_raw.map(process_text, tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iGi7X2m_tbM"
   },
   "source": [
    "Here is the first sequence of each, from the first batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "woQBWAjLsJkr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[START]' 'ã' 'æ§' 'ã' 'å£' 'ã§' 'ç´' 'é³¶' 'ã' 'å§']\n",
      "\n",
      "['ã' 'ã¼' 'ã£' 'ã' 'ã¤' 'ã' 'ã«' 'ã' 'ã­' 'ã³']\n",
      "['ã¼' 'ã£' 'ã' 'ã¤' 'ã' 'ã«' 'ã' 'ã­' 'ã³' 'ã']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-30 21:03:55.640482: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for (ex_context_tok, ex_tar_in), ex_tar_out in train_ds.take(1):\n",
    "  print(context_vocab[ex_context_tok[0, :10].numpy()]) \n",
    "  print()\n",
    "  print(context_vocab[ex_tar_in[0, :10].numpy()]) \n",
    "  print(context_vocab[ex_tar_out[0, :10].numpy()]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNfHIF71ulLu"
   },
   "source": [
    "## The encoder/decoder\n",
    "\n",
    "The following diagrams shows an overview of the model. In both the encoder is on the left, the decoder is on the right. At each time-step the decoder's output is combined with the encoder's output, to predict the next word. \n",
    "\n",
    "The original [left] contains a few extra connections that are intentionally omitted from this tutorial's model [right], as they are generally unnecessary, and difficult to implement. Those missing connections are:\n",
    "\n",
    "1. Feeding the state from the encoder's RNN to the decoder's RNN\n",
    "2. Feeding the attention output back to the RNN's input.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img width=500 src=\"https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg\"/>\n",
    "  </td>\n",
    "  <td>\n",
    "   <img width=380 src=\"https://www.tensorflow.org/images/tutorials/transformer/RNN+attention.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <th colspan=1>The original from <a href=https://arxiv.org/abs/1508.04025v5>Effective Approaches to Attention-based Neural Machine Translation</a></th>\n",
    "  <th colspan=1>This tutorial's model</th>\n",
    "<tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzQWx2saImMV"
   },
   "source": [
    "Before getting into it define constants for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "_a9uNz3-IrF-"
   },
   "outputs": [],
   "source": [
    "UNITS = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blNgVbLSzpsr"
   },
   "source": [
    "### The encoder\n",
    "\n",
    "The goal of the encoder is to process the context sequence into a sequence of vectors that are useful for the decoder as it attempts to predict the next output for each timestep. Since the context sequence is constant, there is no restriction on how information can flow in the encoder, so use a bidirectional-RNN to do the processing:\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img width=500 src=\"https://tensorflow.org/images/tutorials/transformer/RNN-bidirectional.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <th>A bidirectional RNN</th>\n",
    "<tr>\n",
    "</table>\n",
    "\n",
    "The encoder:\n",
    "\n",
    "1. Takes a list of token IDs (from `context_text_processor`).\n",
    "3. Looks up an embedding vector for each token (Using a `layers.Embedding`).\n",
    "4. Processes the embeddings into a new sequence (Using a bidirectional `layers.GRU`).\n",
    "5. Returns the processed sequence. This will be passed to the attention head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "nZ2rI24i3jFg"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, text_processor, units):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.text_processor = text_processor\n",
    "    self.vocab_size = text_processor.vocabulary_size()\n",
    "    self.units = units\n",
    "    \n",
    "    # The embedding layer converts tokens to vectors\n",
    "    self.embedding = tf.keras.layers.Embedding(self.vocab_size, units,\n",
    "                                               mask_zero=True)\n",
    "\n",
    "    # The RNN layer processes those vectors sequentially.\n",
    "    self.rnn = tf.keras.layers.Bidirectional(\n",
    "        merge_mode='sum',\n",
    "        layer=tf.keras.layers.GRU(units,\n",
    "                            # Return the sequence and state\n",
    "                            return_sequences=True,\n",
    "                            recurrent_initializer='glorot_uniform'))\n",
    "\n",
    "  def call(self, x):\n",
    "    shape_checker = ShapeChecker()\n",
    "    shape_checker(x, 'batch s')\n",
    "\n",
    "    # 2. The embedding layer looks up the embedding vector for each token.\n",
    "    x = self.embedding(x)\n",
    "    shape_checker(x, 'batch s units')\n",
    "\n",
    "    # 3. The GRU processes the sequence of embeddings.\n",
    "    x = self.rnn(x)\n",
    "    shape_checker(x, 'batch s units')\n",
    "\n",
    "    # 4. Returns the new sequence of embeddings.\n",
    "    return x\n",
    "\n",
    "  def convert_input(self, texts):\n",
    "    texts = tf.convert_to_tensor(texts)\n",
    "    if len(texts.shape) == 0:\n",
    "      texts = tf.convert_to_tensor(texts)[tf.newaxis]\n",
    "    context = self.text_processor(texts).to_tensor()\n",
    "    context = self(context)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D3SKkaQeGn-Q"
   },
   "source": [
    "Try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "id": "60gSVh05Jl6l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context tokens, shape (batch, s): (64, 196)\n",
      "Encoder output, shape (batch, s, units): (64, 196, 256)\n"
     ]
    }
   ],
   "source": [
    "# Encode the input sequence.\n",
    "encoder = Encoder(context_text_processor, UNITS)\n",
    "ex_context = encoder(ex_context_tok)\n",
    "\n",
    "print(f'Context tokens, shape (batch, s): {ex_context_tok.shape}')\n",
    "print(f'Encoder output, shape (batch, s, units): {ex_context.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45xM_Gl1MgXY"
   },
   "source": [
    "### The attention layer\n",
    "\n",
    "The attention layer lets the decoder access the information extracted by the encoder. It computes a vector from the entire context sequence, and adds that to the decoder's output. \n",
    "\n",
    "The simplest way you could calculate a single vector from the entire sequence would be to take the average across the sequence (`layers.GlobalAveragePooling1D`). An attention layer is similar, but calculates a **weighted** average across the context sequence. Where the weights are calculated from the combination of context and \"query\" vectors.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img width=500 src=\"https://www.tensorflow.org/images/tutorials/transformer/CrossAttention-new-full.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <th colspan=1>The attention layer</th>\n",
    "<tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "id": "-Ql3ymqwD8LS"
   },
   "outputs": [],
   "source": [
    "class CrossAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units, **kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(key_dim=units, num_heads=1, **kwargs)\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "    self.add = tf.keras.layers.Add()\n",
    "\n",
    "  def call(self, x, context):\n",
    "    shape_checker = ShapeChecker()\n",
    " \n",
    "    shape_checker(x, 'batch t units')\n",
    "    shape_checker(context, 'batch s units')\n",
    "\n",
    "    attn_output, attn_scores = self.mha(\n",
    "        query=x,\n",
    "        value=context,\n",
    "        return_attention_scores=True)\n",
    "    \n",
    "    shape_checker(x, 'batch t units')\n",
    "    shape_checker(attn_scores, 'batch heads t s')\n",
    "    \n",
    "    # Cache the attention scores for plotting later.\n",
    "    attn_scores = tf.reduce_mean(attn_scores, axis=1)\n",
    "    shape_checker(attn_scores, 'batch t s')\n",
    "    self.last_attention_weights = attn_scores\n",
    "\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "7y7hjPkNMmHh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context sequence, shape (batch, s, units): (64, 196, 256)\n",
      "Target sequence, shape (batch, t, units): (64, 294, 256)\n",
      "Attention result, shape (batch, t, units): (64, 294, 256)\n",
      "Attention weights, shape (batch, t, s):    (64, 294, 196)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielchan/miniconda3/lib/python3.11/site-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'cross_attention_6' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "attention_layer = CrossAttention(UNITS)\n",
    "\n",
    "# Attend to the encoded tokens\n",
    "embed = tf.keras.layers.Embedding(target_text_processor.vocabulary_size(),\n",
    "                                  output_dim=UNITS, mask_zero=True)\n",
    "ex_tar_embed = embed(ex_tar_in)\n",
    "\n",
    "result = attention_layer(ex_tar_embed, ex_context)\n",
    "\n",
    "print(f'Context sequence, shape (batch, s, units): {ex_context.shape}')\n",
    "print(f'Target sequence, shape (batch, t, units): {ex_tar_embed.shape}')\n",
    "print(f'Attention result, shape (batch, t, units): {result.shape}')\n",
    "print(f'Attention weights, shape (batch, t, s):    {attention_layer.last_attention_weights.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vx9fUhi3Pmwp"
   },
   "source": [
    "The attention weights will sum to `1` over the context sequence, at each location in the target sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "zxyR7cmQPn9P"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.0000001 , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.0000001 ,\n",
       "       1.0000001 , 1.        , 1.0000001 , 1.        , 1.        ,\n",
       "       1.0000001 , 1.        , 1.        , 1.        , 1.0000001 ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.0000001 , 1.        , 1.0000001 , 1.        ,\n",
       "       1.        , 1.0000001 , 1.0000001 , 1.0000001 , 1.        ,\n",
       "       0.9999999 , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.0000001 , 0.99999994, 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.0000001 , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.9999998 , 1.0000001 , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.0000001 , 1.        , 1.        , 1.0000001 ,\n",
       "       1.        , 1.        , 1.0000001 , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 0.99999994,\n",
       "       1.        , 1.        , 1.        , 1.0000001 , 0.99999994,\n",
       "       1.        , 1.        , 0.99999994, 1.0000001 , 1.        ,\n",
       "       1.0000001 , 1.        , 1.0000001 , 1.        , 1.        ,\n",
       "       1.        , 0.9999998 , 1.0000001 , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 0.9999998 ,\n",
       "       1.0000001 , 1.        , 1.        , 1.        , 0.99999994,\n",
       "       1.        , 0.9999999 , 1.        , 1.0000001 , 1.        ,\n",
       "       1.0000001 , 1.0000001 , 0.9999998 , 0.99999994, 1.0000001 ,\n",
       "       1.        , 1.        , 0.9999999 , 0.9999998 , 1.        ,\n",
       "       1.        , 1.        , 1.0000001 , 1.        , 1.        ,\n",
       "       1.0000001 , 0.99999994, 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.0000001 ,\n",
       "       1.0000001 , 1.        , 1.        , 1.        , 1.0000001 ,\n",
       "       0.9999999 , 1.        , 0.99999994, 1.        , 1.        ,\n",
       "       0.99999994, 0.9999999 , 1.        , 1.        , 0.99999994,\n",
       "       1.        , 1.        , 1.        , 1.        , 0.9999999 ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.99999994, 1.0000001 , 0.99999994, 1.        ,\n",
       "       1.        , 1.        , 0.9999998 , 1.0000001 , 1.        ,\n",
       "       1.0000001 , 1.        , 1.        , 1.        , 0.99999994,\n",
       "       1.        , 1.        , 1.        , 0.9999999 , 0.9999998 ,\n",
       "       1.        , 1.0000001 , 1.        , 1.0000001 , 1.        ,\n",
       "       1.        , 1.        , 0.99999994, 1.        , 1.        ,\n",
       "       1.        , 1.        , 0.9999998 , 1.        , 1.        ,\n",
       "       0.9999998 , 1.        , 1.        , 1.0000001 , 0.9999999 ,\n",
       "       0.9999998 , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.0000001 , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.9999998 , 1.        , 1.0000001 , 0.9999999 ,\n",
       "       1.0000001 , 1.0000001 , 1.        , 1.        , 1.        ,\n",
       "       0.99999994, 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.0000001 , 1.        ,\n",
       "       1.        , 1.0000001 , 1.        , 1.        , 1.0000001 ,\n",
       "       1.        , 1.        , 0.99999994, 1.        , 1.        ,\n",
       "       1.        , 1.        , 0.9999998 , 1.        , 1.        ,\n",
       "       0.99999994, 1.0000001 , 1.0000001 , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.0000001 , 0.9999999 , 1.        ,\n",
       "       1.        , 1.        , 1.0000001 , 1.        , 1.        ,\n",
       "       1.        , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
       "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
       "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
       "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
       "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ], dtype=float32)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_layer.last_attention_weights[0].numpy().sum(axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AagyXMH-Jhqt"
   },
   "source": [
    "\n",
    "\n",
    "Here are the attention weights across the context sequences at `t=0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "id": "Rqr8XGsAJlf6"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGzCAYAAACPa3XZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA19UlEQVR4nO3deXRV5b3/8c9JSE5CRgIhCYUwI6BFJdUYGVRIiVykICCD3FtwWbE2ooD+7KVWEa+KWgdKK6D+vLC8BVFUQG0FkRkJM1qFgqAgaEhANANDAiTP7w9/nMsxATOcPPsM79daZy2z9z57f3eQbz48eZ59XMYYIwAAAEvCnC4AAACEFsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCByRJLpdLjzzyiNNl+NzYsWPVpk2bOr83NjbWtwUB8KnVq1fL5XLpzTffdLoU1ALhwwdmzpwpl8ulzMzMavfv2rVLjzzyiA4cOFDte+fOnduwBf5///jHP4IyYDjt5MmTeuSRR7R69WqnSwGsmzt3rlwul1wul9avX19lvzFGrVq1ksvl0k033eRAhfBHhA8fmDdvntq0aaPNmzdr3759Vfbv2rVLU6dO9YvwMXXq1Gr3nTp1Sn/84x+t1GHTyy+/rD179jToNU6ePKmpU6cSPhDSoqKiNH/+/Crb16xZo6+//lput9uBquCvCB/1tH//fm3YsEHPPfeckpOTNW/ePKdLqpOoqCg1atTI6TJ8LiIigqYHWPBv//ZvWrhwoc6ePeu1ff78+crIyFBqaqpDlcEfET7qad68eWrSpIkGDBigYcOGVQkfc+fO1S233CJJuuGGGzzDk6tXr1abNm20c+dOrVmzxrP9+uuv97y3qKhIEyZMUKtWreR2u9WhQwc99dRTqqys9Bxz4MABuVwuPfPMM3rppZfUvn17ud1uXXXVVdqyZYvnuLFjx+qFF16QJM+1XC6XZ391cz527Nih/v37Kz4+XrGxserbt682btxY5f5cLpc++ugjTZo0ScnJyYqJidHNN9+so0ePXvR7984778jlcumf//ynZ9tbb70ll8ulIUOGeB3bpUsXjRgxwmvb3/72N2VkZCg6OlpJSUkaOXKkDh065HVMdXM+jh07pv/4j/9QfHy8EhMTNWbMGH3yySdyuVzVjkJ98803Gjx4sGJjY5WcnKz7779fFRUVkn74/icnJ0uSpk6d6vm+nvteFhQU6LbbblPLli3ldruVlpamQYMGVTsKBgSyUaNG6dixY1q+fLln2+nTp/Xmm2/q1ltvrXL8M888o2uvvVZNmzZVdHS0MjIyqp23sXz5cvXs2VOJiYmKjY3VJZdcoj/84Q8XraW8vFw33XSTEhIStGHDhvrfHHwu+P6pa9m8efM0ZMgQRUZGatSoUZo1a5a2bNmiq666SpLUu3dv3XPPPZoxY4b+8Ic/qEuXLpJ++GE6ffp0jR8/XrGxsXrwwQclSSkpKZJ+GMq/7rrr9M033+jOO+9Uenq6NmzYoMmTJ+vw4cOaPn26Vx3z589XaWmp7rzzTrlcLj399NMaMmSIvvzyS0VEROjOO+9Ufn6+li9frv/5n//5yfvauXOnevXqpfj4eD3wwAOKiIjQiy++qOuvv15r1qypMr9l/PjxatKkiaZMmaIDBw5o+vTpuvvuu/X6669f8Bo9e/aUy+XS2rVr1a1bN0nSunXrFBYW5vW746NHj2r37t26++67Pdsef/xxPfTQQxo+fLh+85vf6OjRo/rLX/6i3r17a8eOHUpMTKz2mpWVlRo4cKA2b96su+66S507d9aSJUs0ZsyYao+vqKhQTk6OMjMz9cwzz+jDDz/Us88+q/bt2+uuu+5ScnKyZs2apbvuuks333yzJzSdu5+hQ4dq586dGj9+vNq0aaMjR45o+fLlOnjwYJ0nwgL+qE2bNsrKytJrr72m/v37S5Lef/99FRcXa+TIkZoxY4bX8X/+85/1q1/9SqNHj9bp06e1YMEC3XLLLXrvvfc0YMAAST/0oZtuukndunXTo48+KrfbrX379umjjz66YB2nTp3SoEGDtHXrVn344YeeXgw/Y1BnW7duNZLM8uXLjTHGVFZWmpYtW5p7773X67iFCxcaSWbVqlVVznHppZea6667rsr2//qv/zIxMTHm888/99r+n//5nyY8PNwcPHjQGGPM/v37jSTTtGlT891333mOW7JkiZFk3n33Xc+23Nxcc6E/cklmypQpnq8HDx5sIiMjzRdffOHZlp+fb+Li4kzv3r092+bMmWMkmezsbFNZWenZPnHiRBMeHm6Kioqqvd759z98+HDP1927dze33HKLkWT+9a9/GWOMefvtt40k88knnxhjjDlw4IAJDw83jz/+uNe5Pv30U9OoUSOv7WPGjDGtW7f2fP3WW28ZSWb69OmebRUVFaZPnz5GkpkzZ47XeyWZRx991Os6V155pcnIyPB8ffTo0SrfP2OM+f77740k86c//emi3wMgkJ3rAVu2bDF//etfTVxcnDl58qQxxphbbrnF3HDDDcYYY1q3bm0GDBjged+5Y845ffq0ueyyy0yfPn08255//nkjyRw9evSC11+1apWRZBYuXGhKS0vNddddZ5o1a2Z27Njhw7uEr/Frl3qYN2+eUlJSdMMNN0j64VcXI0aM0IIFCzzD8nW1cOFC9erVS02aNNG3337reWVnZ6uiokJr1671On7EiBFq0qSJ5+tevXpJkr788staX7uiokIffPCBBg8erHbt2nm2p6Wl6dZbb9X69etVUlLi9Z5x48Z5/RqnV69eqqio0FdffXXRa/Xq1Uvr1q2TJJWWluqTTz7RuHHj1KxZM8/2devWKTExUZdddpkk6e2331ZlZaWGDx/u9b1JTU1Vx44dtWrVqgteb+nSpYqIiNAdd9zh2RYWFqbc3NwLvue3v/1tlZpr8n2Njo5WZGSkVq9ere+///4njwcC3fDhw3Xq1Cm99957Ki0t1XvvvVftr1ykH/5+nPP999+ruLhYvXr10vbt2z3bz41gLlmyxOvXzdUpLi5Wv379tHv3bq1evVpXXHFFve8HDYfwUUcVFRVasGCBbrjhBu3fv1/79u3Tvn37lJmZqcLCQq1YsaJe59+7d6+WLl2q5ORkr1d2drYk6ciRI17Hp6ene319LojU5Yfe0aNHdfLkSV1yySVV9nXp0kWVlZVV5lbU9fq9evXS4cOHtW/fPm3YsEEul0tZWVleoWTdunXq0aOHwsJ++N917969MsaoY8eOVb4///rXv6p8b8731VdfKS0tTY0bN/ba3qFDh2qPj4qK8szpOP/eavJ9dbvdeuqpp/T+++8rJSVFvXv31tNPP62CgoKffC8QiM71qPnz5+vtt99WRUWFhg0bVu2x7733nq655hpFRUUpKSnJ8yvM4uJizzEjRoxQjx499Jvf/EYpKSkaOXKk3njjjWqDyIQJE7RlyxZ9+OGHuvTSSxvsHuEbzPmoo5UrV+rw4cNasGCBFixYUGX/vHnz1K9fvzqfv7KyUr/85S/1wAMPVLu/U6dOXl+Hh4dXe5wxps411EZdr9+zZ09J0tq1a/Xll1+qe/fuiomJUa9evTRjxgwdP35cO3bs0OOPP+55T2VlpVwul95///1qr+vLB4Nd6L5qasKECRo4cKAWL16sZcuW6aGHHtK0adO0cuVKXXnllT6qEvAft956q+644w4VFBSof//+1c6/WrdunX71q1+pd+/emjlzptLS0hQREaE5c+Z4LdeNjo7W2rVrtWrVKv3973/X0qVL9frrr6tPnz764IMPvP5+Dho0SAsWLNCTTz6pV1991fOPFfgnwkcdzZs3T82bN/esIDnf22+/rUWLFmn27NmKjo72+nXEj11oX/v27XX8+HHPSIcvXKyO8yUnJ6tx48bVPh9j9+7dCgsLU6tWrXxSU3p6utLT07Vu3Tp9+eWXnl8X9e7dW5MmTdLChQtVUVGh3r17e97Tvn17GWPUtm3bKiHsp7Ru3VqrVq3SyZMnvUY/qns+S0391Pe1ffv2uu+++3Tfffdp7969uuKKK/Tss8/qb3/7W52vCfirm2++WXfeeac2btx4wQnnb731lqKiorRs2TKvpfBz5sypcmxYWJj69u2rvn376rnnntMTTzyhBx98UKtWrfLqj4MHD1a/fv00duxYxcXFadasWb6/OfgM0bAOTp06pbfffls33XSThg0bVuV19913q7S0VO+8844kKSYmRtIPS2d/LCYmptrtw4cPV15enpYtW1ZlX1FRUZW19DVxsTrOFx4ern79+mnJkiVeS0ILCws1f/589ezZU/Hx8bW+/oX06tVLK1eu1ObNmz3h44orrlBcXJyefPJJzzK8c4YMGaLw8HBNnTq1ysiKMUbHjh274LVycnJ05swZvfzyy55tlZWV1YbImjoXYn78fT158qTKysq8trVv315xcXEqLy+v8/UAfxYbG6tZs2bpkUce0cCBA6s9Jjw8XC6Xy2tu3IEDB7R48WKv47777rsq7z03l6O6v0O//vWvNWPGDM2ePVu///3v634TaHCMfNTBO++8o9LSUv3qV7+qdv8111zjeeDYiBEjdMUVVyg8PFxPPfWUiouL5Xa71adPHzVv3lwZGRmaNWuWHnvsMXXo0EHNmzdXnz599H/+z//RO++8o5tuukljx45VRkaGTpw4oU8//VRvvvmmDhw4oGbNmtWq7nM/wO+55x7l5OQoPDxcI0eOrPbYxx57zLO+/ne/+50aNWqkF198UeXl5Xr66adr9w37Cb169dK8efPkcrk8v4YJDw/Xtddeq2XLlun6669XZGSk5/j27dvrscce0+TJk3XgwAENHjxYcXFx2r9/vxYtWqRx48bp/vvvr/ZagwcP1tVXX6377rtP+/btU+fOnfXOO+94mlxNR4fOFx0dra5du+r1119Xp06dlJSUpMsuu0xnz55V3759NXz4cHXt2lWNGjXSokWLVFhYeMHvOxAMLrR0/ZwBAwboueee04033qhbb71VR44c0QsvvKAOHTp4Pffn0Ucf1dq1azVgwAC1bt1aR44c0cyZM9WyZUtPr/ixu+++WyUlJXrwwQeVkJDwk88EgUOcXGoTqAYOHGiioqLMiRMnLnjM2LFjTUREhPn222+NMca8/PLLpl27diY8PNxr2W1BQYEZMGCAiYuLM5K8lt2WlpaayZMnmw4dOpjIyEjTrFkzc+2115pnnnnGnD592hjzv0ttq1vOqR8t/zx79qwZP368SU5ONi6Xy2vZ7Y+PNcaY7du3m5ycHBMbG2saN25sbrjhBrNhwwavY85fZne+c8vfqlte/GM7d+40kkyXLl28tj/22GNGknnooYeqfd9bb71levbsaWJiYkxMTIzp3Lmzyc3NNXv27PEc8+Oltsb8sDT21ltvNXFxcSYhIcGMHTvWfPTRR0aSWbBggdd7Y2Jiqlx3ypQpVZYsb9iwwWRkZJjIyEjP9/Lbb781ubm5pnPnziYmJsYkJCSYzMxM88Ybb/zk9wQIFBfqAT/246W2r7zyiunYsaNxu92mc+fOZs6cOVX+bq1YscIMGjTItGjRwkRGRpoWLVqYUaNGeT2C4Pyltud74IEHjCTz17/+1Ud3Cl9yGWNpRiLgxxYvXqybb75Z69evV48ePZwuBwCCGuEDIefUqVNezxioqKhQv379tHXrVhUUFHjtAwD4HnM+EHLGjx+vU6dOKSsrS+Xl5Xr77be1YcMGPfHEEwQPALCAkQ+EnPnz5+vZZ5/Vvn37VFZWpg4dOuiuu+7y+uwYAEDDIXwAAACreM4HAACwivABAACs8rsJp5WVlcrPz1dcXFydHvgEoP6MMSotLVWLFi0C5jMy6B2As2rTN/wufOTn5/vsc0MA1M+hQ4fUsmVLp8uoEXoH4B9q0jf8LnzExcVJku76oL/cMREOV4NA8nEP/rXrK2d1Ruv1D8/fx0BwrtavtrdRfGxgjNbAeTd3+rnTJQSN2vQNvwsf54ZL3TERcscSPlBzjRhq953/vwYukH59ca7W+NgwxceF/8TRwA8aufg54zO16Bv88wAAAFhF+AAAAFb53a9dzrmv6TbFx5GNAt3QlplOlwAgAOW0uNzpEtCA+OkOAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKzy29Uuw3YPVaMYt9NloL4+cLoA34nut9/pEoCQsSz/E6dL8BlW7lTFyAcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsMpvV7t0b/q13LF81DFq77PuFU6XACAAsSrFHkY+AACAVbUOH998843+/d//XU2bNlV0dLR+/vOfa+vWrZ79xhg9/PDDSktLU3R0tLKzs7V3716fFg0g8NA7AJxTq/Dx/fffq0ePHoqIiND777+vXbt26dlnn1WTJk08xzz99NOaMWOGZs+erU2bNikmJkY5OTkqKyvzefEAAgO9A8D5ajXn46mnnlKrVq00Z84cz7a2bdt6/tsYo+nTp+uPf/yjBg0aJEl69dVXlZKSosWLF2vkyJE+KhtAIKF3ADhfrUY+3nnnHf3iF7/QLbfcoubNm+vKK6/Uyy+/7Nm/f/9+FRQUKDs727MtISFBmZmZysvLq/ac5eXlKikp8XoBCC70DgDnq9XIx5dffqlZs2Zp0qRJ+sMf/qAtW7bonnvuUWRkpMaMGaOCggJJUkpKitf7UlJSPPt+bNq0aZo6dWqV7f/Y0F1hUVG1KQ/4wXSnCwh8re9d59Pz2ewdQF0F0+fJOKFvWtcaH1urkY/Kykp1795dTzzxhK688kqNGzdOd9xxh2bPnl3rIs+ZPHmyiouLPa9Dhw7V+VwA/BO9A8D5ahU+0tLS1LWrd7Lp0qWLDh48KElKTU2VJBUWFnodU1hY6Nn3Y263W/Hx8V4vAMGF3gHgfLUKHz169NCePXu8tn3++edq3bq1pB8mkKWmpmrFihWe/SUlJdq0aZOysrJ8UC6AQETvAHC+Ws35mDhxoq699lo98cQTGj58uDZv3qyXXnpJL730kiTJ5XJpwoQJeuyxx9SxY0e1bdtWDz30kFq0aKHBgwc3RP0AAgC9A8D5ahU+rrrqKi1atEiTJ0/Wo48+qrZt22r69OkaPXq055gHHnhAJ06c0Lhx41RUVKSePXtq6dKlimLyKBCy6B0AzucyxhinizhfSUmJEhISdPPy2xQRE+l0OQhS3/U45nQJfu2sOaPVWqLi4uKAmUtxrnd8/3k7xceFO10OghSf/3JhtekbfLYLAACwivABAACsInwAAACrCB8AAMCqWq12semzwhSFN2aWOxrIW2nWLtVy6GfWrgWgYdl6BHuwT2xl5AMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWOW3q12Gtf9YUbERTpeBALCum9vpEgAEmGBfTeLvGPkAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFb57WqXv23PUlg0n+2CGnjF6QKc0/H2LU6XAAQkW5/R4o/8YaUPIx8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCq/Xe0SlXBK4Y2N02UA9dZy6GdOlwAgwPjDipSGxMgHAACwivABAACsInwAAACrCB8AAMAqv51w2q7pd4qIiXS6DISQsusKnC4BQIAJ9omhDYWRDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgld+udvni26YKPxnldBkIJW8mN9ipWw37tMHODcA5y/I/abBzB/NKGkY+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVfrva5cw3saqIquVqF5eRjOunj5F++jjAh754PsvpEmqlsqxM+s8lTpcBhLSGXEnTEEpKK9SkU82OZeQDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFjlt6tdrr5qjyJiIp0uA0HoSFaR0yX4vbPmjL5yugjAzwTzZ634wllzRtKXNTqWkQ8AAGBVrcLHI488IpfL5fXq3LmzZ39ZWZlyc3PVtGlTxcbGaujQoSosLPR50QACC70DwPlqPfJx6aWX6vDhw57X+vXrPfsmTpyod999VwsXLtSaNWuUn5+vIUOG+LRgAIGJ3gHgnFrP+WjUqJFSU1OrbC8uLtYrr7yi+fPnq0+fPpKkOXPmqEuXLtq4caOuueaa+lcLIGDROwCcU+uRj71796pFixZq166dRo8erYMHD0qStm3bpjNnzig7O9tzbOfOnZWenq68vLwLnq+8vFwlJSVeLwDBh94B4JxajXxkZmZq7ty5uuSSS3T48GFNnTpVvXr10meffaaCggJFRkYqMTHR6z0pKSkqKCi44DmnTZumqVOnVtmeFHFC7sjTtSkP8NiVcdbpEnAem70DqA9WtNhRq/DRv39/z39369ZNmZmZat26td544w1FR0fXqYDJkydr0qRJnq9LSkrUqlWrOp0LgH+idwA4X72W2iYmJqpTp07at2+fUlNTdfr0aRUVFXkdU1hYWO3vec9xu92Kj4/3egEIbvQOILTVK3wcP35cX3zxhdLS0pSRkaGIiAitWLHCs3/Pnj06ePCgsrIC6+PEATQsegcQ2mr1a5f7779fAwcOVOvWrZWfn68pU6YoPDxco0aNUkJCgm6//XZNmjRJSUlJio+P1/jx45WVlcVsdSDE0TsAnK9W4ePrr7/WqFGjdOzYMSUnJ6tnz57auHGjkpOTJUnPP/+8wsLCNHToUJWXlysnJ0czZ86sU2H/2H65wqKj6vReQLOdLsB5HX+72ekSPGz2DqA+luV/4nQJjrI14dZljDFWrlRDJSUlSkhIUKvpjxI+gHqoT/g4a85otZaouLg4YOZSnOsd33/eTvFx4U6XAwSk+oSP2vQNPtsFAABYRfgAAABWET4AAIBVhA8AAGBVrT9YzpbuXQ8oIibS6TIQxIp6fut0CQACEI9grz9GPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVX672uXE2Qg1OstqFzScRqtbVNl29vp8ByoBEEiq+/wXVsDUDiMfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqv13tcvD7JgovdztdBkLN2018erqfDdnp0/MB8E/VrYCpq1BYOcPIBwAAsIrwAQAArCJ8AAAAqwgfAADAKr+dcJre5Hs1imHCKezi8eoA6iIUJon6EiMfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqv13tcmvqZjWOC3e6DDjo/3Zq43QJAAIQK0/8HyMfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqv13t8vKhnny2S6hb7nQB/8v9ywNOlwCghpblf+J0CZJYdXMxjHwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKv8drXLyTORCj8d6XQZgCTpxN87XXBf0oDPLVYCIFBcbNVNqK+EYeQDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFjlt6td5nT9m2LjyEbB6HfpPZwuAUAACvUVIsGkXj/dn3zySblcLk2YMMGzraysTLm5uWratKliY2M1dOhQFRYW1rdOAEGCvgGgzuFjy5YtevHFF9WtWzev7RMnTtS7776rhQsXas2aNcrPz9eQIUPqXSiAwEffACDVMXwcP35co0eP1ssvv6wmTZp4thcXF+uVV17Rc889pz59+igjI0Nz5szRhg0btHHjRp8VDSDw0DcAnFOn8JGbm6sBAwYoOzvba/u2bdt05swZr+2dO3dWenq68vLyqj1XeXm5SkpKvF4Ago8v+4ZE7wACWa0nnC5YsEDbt2/Xli1bquwrKChQZGSkEhMTvbanpKSooKCg2vNNmzZNU6dOrbL9oUMDFRHD49WDUew6pyvwdrzXEadLCHq+7hvShXsHgtfFHlduG5Nf66dWIx+HDh3Svffeq3nz5ikqKsonBUyePFnFxcWe16FDh3xyXgD+oSH6hkTvAAJZrcLHtm3bdOTIEXXv3l2NGjVSo0aNtGbNGs2YMUONGjVSSkqKTp8+raKiIq/3FRYWKjU1tdpzut1uxcfHe70ABI+G6BsSvQMIZLX6tUvfvn316aefem277bbb1LlzZ/3+979Xq1atFBERoRUrVmjo0KGSpD179ujgwYPKysryXdUAAgZ9A8CP1Sp8xMXF6bLLLvPaFhMTo6ZNm3q233777Zo0aZKSkpIUHx+v8ePHKysrS9dcc43vqgYQMOgbAH7M5084ff755xUWFqahQ4eqvLxcOTk5mjlzpq8vAyCI0DeA0OIyxhinizhfSUmJEhIS9M4/2ysmLtzpcmDRtHbdfvogWHHWnNFqLVFxcXHAzKU41zu+/7yd4ukdIYNVJ/6jNn2DD08BAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFb5fKmtr+wub6HoCL8tDw1g0K5jP3nMkq5NLVQCIFDU5PNeWBHjfxj5AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABW+e1yki9ONZc7PMLpMuBnum6r2XG7Ms42bCEAAkZNVsScw8oYOxj5AAAAVhE+AACAVYQPAABgFeEDAABY5bcTTnObrVNcHNko2NyR3tPpEgAEGCaBBh9+ugMAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAq/x2tcu/7/y1whu7nS4Dvvae0wU0jKY37XG6BCBo1ebx6IEklFfxMPIBAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKzy29Uuue1XKTrWb8uDJa9e0srpEgAEoFBeSRIIGPkAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFb57XKS/z7YQ41i+GyXkPeB0wX8r+h++50uAUAN+dPnwbDypipGPgAAgFWEDwAAYBXhAwAAWEX4AAAAVvnthNOysxEKPxPhdBmAx6m/d6p2e9KAzy1XAiCQVDf5NdQnoTLyAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACs8tvVLu7ws2rUKNzpMgAvjXO+dLoEAAEo1Fe3/BgjHwAAwKpahY9Zs2apW7duio+PV3x8vLKysvT+++979peVlSk3N1dNmzZVbGyshg4dqsLCQp8XDSCw0DsAnK9W4aNly5Z68skntW3bNm3dulV9+vTRoEGDtHPnTknSxIkT9e6772rhwoVas2aN8vPzNWTIkAYpHEDgoHcAOJ/LGGPqc4KkpCT96U9/0rBhw5ScnKz58+dr2LBhkqTdu3erS5cuysvL0zXXXFOj85WUlCghIUFXL7pXjWLc9SkN8LlQmfNx1pzRai1RcXGx4uPjG+QaDdU7vv+8neLjmC8G/xIKcz5q0zfqPOejoqJCCxYs0IkTJ5SVlaVt27bpzJkzys7O9hzTuXNnpaenKy8v74LnKS8vV0lJidcLQPCidwCo9WqXTz/9VFlZWSorK1NsbKwWLVqkrl276uOPP1ZkZKQSExO9jk9JSVFBQcEFzzdt2jRNnTq1yvb8r5MUFh1V2/KAhvVyU6cruKBOd2xxuoSLstU7AH9U3ee7+AsnRmVqPfJxySWX6OOPP9amTZt01113acyYMdq1a1edC5g8ebKKi4s9r0OHDtX5XAD8F70DwDm1HvmIjIxUhw4dJEkZGRnasmWL/vznP2vEiBE6ffq0ioqKvP4FU1hYqNTU1Auez+12y+1mbgcQ7OgdAM6p93M+KisrVV5eroyMDEVERGjFihWefXv27NHBgweVlZVV38sACDL0DiB01WrkY/Lkyerfv7/S09NVWlqq+fPna/Xq1Vq2bJkSEhJ0++23a9KkSUpKSlJ8fLzGjx+vrKysGs9WBxCc6B0Azler8HHkyBH9+te/1uHDh5WQkKBu3bpp2bJl+uUvfylJev755xUWFqahQ4eqvLxcOTk5mjlzZoMUDiBw0DsAnK/ez/nwtXNr9e9ZP0ju2Ainy0GA2XGl0xUEBxvP+fA1nvOB+giF53A0NCvP+QAAAKgLwgcAALCK8AEAAKwifAAAAKsIHwAAwKpaP+HUljMmXGGGGeuoncu2S591r3C6DAABZln+J6x4sYiRDwAAYBXhAwAAWEX4AAAAVhE+AACAVX474XTZ/s4KbxzldBkIRAudLuDCWt/yT6dLAHABy/I/cbqECwq2ybCMfAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAq/x2tcu2axYoPo7HqweqYJuZDcAOekdoYOQDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFjlt6tdrlx/q8L4bJfA9ZrTBQSOdqM+droEwG/48+er+JNAXxXEyAcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsMpvV7ucPR6psIpIp8sAGtznL13ts3N1GrfZZ+cC4L98vSrI9uoZRj4AAIBVhA8AAGAV4QMAAFhF+AAAAFb57YTTpLQShTcud7oMoEaa3rTH6RIABJhAf0R6fTDyAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACs8tvVLre13aDoWL8tDw54s0tzp0sAEGBCeUWJP2PkAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABY5bfLSaav+DeFRUU5XQb8yXSnCwgdre9d53QJgE8sy//E6RJCRt+0rjU+lpEPAABgVa3Cx7Rp03TVVVcpLi5OzZs31+DBg7Vnj/eneZaVlSk3N1dNmzZVbGyshg4dqsLCQp8WDSCw0DsAnK9W4WPNmjXKzc3Vxo0btXz5cp05c0b9+vXTiRMnPMdMnDhR7777rhYuXKg1a9YoPz9fQ4YM8XnhAAIHvQPA+VzGGFPXNx89elTNmzfXmjVr1Lt3bxUXFys5OVnz58/XsGHDJEm7d+9Wly5dlJeXp2uuueYnz1lSUqKEhAS1fuox5nwADml97zqt1hIVFxcrPj7e5+dvyN7x/eftFB8X7vOaAVxc37SuNe4b9ZrzUVxcLElKSkqSJG3btk1nzpxRdna255jOnTsrPT1deXl51Z6jvLxcJSUlXi8AwY3eAYS2Oq92qays1IQJE9SjRw9ddtllkqSCggJFRkYqMTHR69iUlBQVFBRUe55p06Zp6tSpVba7Wx5XeOMzdS0P8As/G7LT6RLq5GwDnruhewcQ6AL382hq/jO7ziMfubm5+uyzz7RgwYK6nkKSNHnyZBUXF3tehw4dqtf5APg3egeAOo183H333Xrvvfe0du1atWzZ0rM9NTVVp0+fVlFRkde/YAoLC5Wamlrtudxut9xud13KABBg6B0ApFqOfBhjdPfdd2vRokVauXKl2rZt67U/IyNDERERWrFihWfbnj17dPDgQWVlZfmmYgABh94B4Hy1GvnIzc3V/PnztWTJEsXFxXl+F5uQkKDo6GglJCTo9ttv16RJk5SUlKT4+HiNHz9eWVlZNZqtDiA40TsAnK9W4WPWrFmSpOuvv95r+5w5czR27FhJ0vPPP6+wsDANHTpU5eXlysnJ0cyZM31SLIDARO8AcL56PeejIZxbq3/n2qFyx0Y4XQ4C1K6MhlyvEfzOmjMN+pyPhsBzPuALgbvSxHm16Rt8tgsAALCK8AEAAKwifAAAAKsIHwAAwKo6P169oW080kbhx3mAEOro7/YulTTgc3sXA9CgluV/YuU6oT6xlZEPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGCV3652ubfdSjXmEckh7f92auN0CQACUKivJAkEjHwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKv8drXLu8euUERZpNNlwEFJHzXcub/rcazhTg7AUQ35+SyspPENRj4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFV+u9ol/0ScGsntdBkIVh/E1/ot0f32N0AhAAJJXVbSsEKmKkY+AACAVYQPAABgFeEDAABYRfgAAABW+e2E00R3mSLclU6XgRBSdl2B0yUACDBMJq0bRj4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFV+u9pl9z/TFRYV5XQZCCXT2zhdgd9ofe86p0sAAkJdHrcerPqmda3xsYx8AAAAqwgfAADAKsIHAACwivABAACsInwAAACr/Ha1S2XiGSk63OkyAMd1HLPN+jXPWr8iAF9y5jNnztT4SEY+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVfrva5aaun8odG+F0GQhAn3WvcLoEAAHImRUioYmRDwAAYFWtw8fatWs1cOBAtWjRQi6XS4sXL/bab4zRww8/rLS0NEVHRys7O1t79+71Vb0AAhB9A8D5ah0+Tpw4ocsvv1wvvPBCtfuffvppzZgxQ7Nnz9amTZsUExOjnJwclZWV1btYAIGJvgHgfLWe89G/f3/179+/2n3GGE2fPl1//OMfNWjQIEnSq6++qpSUFC1evFgjR46sX7UAAhJ9A8D5fDrnY//+/SooKFB2drZnW0JCgjIzM5WXl1fte8rLy1VSUuL1AhA66tI3JHoHEMh8utqloKBAkpSSkuK1PSUlxbPvx6ZNm6apU6dW2f7e9isUFh3ly/IQKl50ugDndbxzs9Ml1Fhd+oZ04d4B1NWy/E+cLsFRNlf7OL7aZfLkySouLva8Dh065HRJAAIAvQMIXD4NH6mpqZKkwsJCr+2FhYWefT/mdrsVHx/v9QIQOurSNyR6BxDIfBo+2rZtq9TUVK1YscKzraSkRJs2bVJWVpYvLwUgSNA3gNBT6zkfx48f1759+zxf79+/Xx9//LGSkpKUnp6uCRMm6LHHHlPHjh3Vtm1bPfTQQ2rRooUGDx7sy7oBBBD6BoDz1Tp8bN26VTfccIPn60mTJkmSxowZo7lz5+qBBx7QiRMnNG7cOBUVFalnz55aunSpoqJqN3nUhBmZMFPb8oCQ1emOLU6XcEG2+gaA2nPisfIuY4xf/YQvKSlRQkKCWv5lKqtdgFrwZfg4a85otZaouLg4YOZSnOsd33/eTvFx4U6XAwQMX4WP2vQNx1e7AACA0EL4AAAAVhE+AACAVYQPAABglU8fr+5LjRqfVVjjs06XAVjR7tYdTpcAIAA5sVLFFxj5AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABW+e1ql5SmxWoUU+Z0GYBHdL/9TpcAIAAF6oqUhsTIBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwym9Xu3xzqKnCoqOcLgP4Xy83c7qCi+p0xxanSwBQjWX5nzhdwkU5sRqHkQ8AAGAV4QMAAFhF+AAAAFYRPgAAgFV+O+G0cdIphTeudLoMoF5+NmSn0yUACDCh8Dh2Rj4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFV+u9olMqJC4REVTpcB1FjywN1OlwAgAIXC6pYfY+QDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFjlt6tdVl7+huLjyEaB7KafZThdAoAAFIqrP0INP90BAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFV+u9rliW8vl7sswukyUA8ZHxur19t2hcvq9QA0jGX5n1i7FitrnMHIBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwym9Xu7y18SqFRUc5XQYCyQtOF+DfOuZucroEwO/YXFkTqBpiRVCDjXy88MILatOmjaKiopSZmanNmzc31KUABAn6BhAaGiR8vP7665o0aZKmTJmi7du36/LLL1dOTo6OHDnSEJcDEAToG0DoaJDw8dxzz+mOO+7Qbbfdpq5du2r27Nlq3Lix/vu//7shLgcgCNA3gNDh8zkfp0+f1rZt2zR58mTPtrCwMGVnZysvL6/K8eXl5SovL/d8XVxcLEmqLCvzdWlASDtrztT8WP1wrDF2nlJb274hXbh3lByvbNhigRBT095Rm77h8/Dx7bffqqKiQikpKV7bU1JStHv37irHT5s2TVOnTq2y/ZsHH/d1aUBIO1SH95SWliohIcHntfxYbfuGdOHe0br7gYYoEQhhX9bq6Jr0DcdXu0yePFmTJk3yfF1UVKTWrVvr4MGDVpqeU0pKStSqVSsdOnRI8fHxTpfTIELhHqXgvE9jjEpLS9WiRQunS7mgUOwdwfj/WnVC4T6D8R5r0zd8Hj6aNWum8PBwFRYWem0vLCxUampqlePdbrfcbneV7QkJCUHzB3Ix8fHxQX+foXCPUvDdp80f4LXtG1Jo945g+3/tQkLhPoPtHmvaN3w+4TQyMlIZGRlasWKFZ1tlZaVWrFihrKwsX18OQBCgbwChpUF+7TJp0iSNGTNGv/jFL3T11Vdr+vTpOnHihG677baGuByAIEDfAEJHg4SPESNG6OjRo3r44YdVUFCgK664QkuXLq0ymaw6brdbU6ZMqXY4NZiEwn2Gwj1KoXOfDa0+fUMKjT+HULhHKTTuMxTu8WJcxtZaOgAAAPHBcgAAwDLCBwAAsIrwAQAArCJ8AAAAqwgfAADAKr8LHy+88ILatGmjqKgoZWZmavPmzU6XVGePPPKIXC6X16tz586e/WVlZcrNzVXTpk0VGxuroUOHVnnCoz9au3atBg4cqBYtWsjlcmnx4sVe+40xevjhh5WWlqbo6GhlZ2dr7969Xsd89913Gj16tOLj45WYmKjbb79dx48ft3gXF/dT9zh27Ngqf7Y33nij1zH+fo/BJJj6hhScvSMU+oZE76gpvwofr7/+uiZNmqQpU6Zo+/btuvzyy5WTk6MjR444XVqdXXrppTp8+LDntX79es++iRMn6t1339XChQu1Zs0a5efna8iQIQ5WWzMnTpzQ5ZdfrhdeeKHa/U8//bRmzJih2bNna9OmTYqJiVFOTo7Kzvuk4tGjR2vnzp1avny53nvvPa1du1bjxo2zdQs/6afuUZJuvPFGrz/b1157zWu/v99jsAjGviEFX+8Ihb4h0TtqzPiRq6++2uTm5nq+rqioMC1atDDTpk1zsKq6mzJlirn88sur3VdUVGQiIiLMwoULPdv+9a9/GUkmLy/PUoX1J8ksWrTI83VlZaVJTU01f/rTnzzbioqKjNvtNq+99poxxphdu3YZSWbLli2eY95//33jcrnMN998Y632mvrxPRpjzJgxY8ygQYMu+J5Au8dAFmx9w5jg7x2h0DeMoXdcjN+MfJw+fVrbtm1Tdna2Z1tYWJiys7OVl5fnYGX1s3fvXrVo0ULt2rXT6NGjdfDgQUnStm3bdObMGa/77dy5s9LT0wP6fvfv36+CggKv+0pISFBmZqbnvvLy8pSYmKhf/OIXnmOys7MVFhamTZs2Wa+5rlavXq3mzZvrkksu0V133aVjx4559gXLPfq7YO0bUmj1jlDqGxK9Q/KjX7t8++23qqioqPIo5ZSUFBUUFDhUVf1kZmZq7ty5Wrp0qWbNmqX9+/erV69eKi0tVUFBgSIjI5WYmOj1nkC+X0me2i/251hQUKDmzZt77W/UqJGSkpIC5t5vvPFGvfrqq1qxYoWeeuoprVmzRv3791dFRYWk4LjHQBCMfUMKvd4RKn1Donec0yCf7YIf9O/f3/Pf3bp1U2Zmplq3bq033nhD0dHRDlaG+ho5cqTnv3/+85+rW7duat++vVavXq2+ffs6WBmCAb0jeNE7fuA3Ix/NmjVTeHh4lRnbhYWFSk1Ndagq30pMTFSnTp20b98+paam6vTp0yoqKvI6JtDv91ztF/tzTE1NrTIZ8OzZs/ruu+8C9t7btWunZs2aad++fZKC8x79USj0DSn4e0eo9g0pdHuH34SPyMhIZWRkaMWKFZ5tlZWVWrFihbKyshyszHeOHz+uL774QmlpacrIyFBERITX/e7Zs0cHDx4M6Ptt27atUlNTve6rpKREmzZt8txXVlaWioqKtG3bNs8xK1euVGVlpTIzM63X7Atff/21jh07prS0NEnBeY/+KBT6hhT8vSNU+4YUwr3D6Rmv51uwYIFxu91m7ty5ZteuXWbcuHEmMTHRFBQUOF1andx3331m9erVZv/+/eajjz4y2dnZplmzZubIkSPGGGN++9vfmvT0dLNy5UqzdetWk5WVZbKyshyu+qeVlpaaHTt2mB07dhhJ5rnnnjM7duwwX331lTHGmCeffNIkJiaaJUuWmH/+859m0KBBpm3btubUqVOec9x4443myiuvNJs2bTLr1683HTt2NKNGjXLqlqq42D2Wlpaa+++/3+Tl5Zn9+/ebDz/80HTv3t107NjRlJWVec7h7/cYLIKtbxgTnL0jFPqGMfSOmvKr8GGMMX/5y19Menq6iYyMNFdffbXZuHGj0yXV2YgRI0xaWpqJjIw0P/vZz8yIESPMvn37PPtPnTplfve735kmTZqYxo0bm5tvvtkcPnzYwYprZtWqVUZSldeYMWOMMT8sm3vooYdMSkqKcbvdpm/fvmbPnj1e5zh27JgZNWqUiY2NNfHx8ea2224zpaWlDtxN9S52jydPnjT9+vUzycnJJiIiwrRu3drccccdVX7Y+fs9BpNg6hvGBGfvCIW+YQy9o6Zcxhhjb5wFAACEOr+Z8wEAAEID4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABW/T8TCfG5fYhG2AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attention_weights = attention_layer.last_attention_weights\n",
    "mask=(ex_context_tok != 0).numpy()\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.pcolormesh(mask*attention_weights[:, 0, :])\n",
    "plt.title('Attention weights')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pcolormesh(mask)\n",
    "plt.title('Mask');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Eil-C_NN1rp"
   },
   "source": [
    "Because of the small-random initialization the attention weights are initially all close to `1/(sequence_length)`. The model will learn to make these less uniform as training progresses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQ638eHN4iCK"
   },
   "source": [
    "### The decoder\n",
    "\n",
    "The decoder's job is to generate predictions for the next token at each location in the target sequence.\n",
    "\n",
    "1. It looks up embeddings for each token in the target sequence.\n",
    "2. It uses an RNN to process the target sequence, and keep track of what it has generated so far.\n",
    "3. It uses RNN output as the \"query\" to the attention layer, when attending to the encoder's output.\n",
    "4. At each location in the output it predicts the next token.\n",
    "\n",
    "When training, the model predicts the next word at each location. So it's important that the information only flows in one direction through the model. The decoder uses a unidirectional (not bidirectional) RNN to process the target sequence.\n",
    "\n",
    "When running inference with this model it produces one word at a time, and those are fed back into the model.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img width=500 src=\"https://tensorflow.org/images/tutorials/transformer/RNN.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <th>A unidirectional RNN</th>\n",
    "<tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZsQJMqNmg_L"
   },
   "source": [
    "Here is the `Decoder` class' initializer. The initializer creates all the necessary layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "erYvHIgAl8kh"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  @classmethod\n",
    "  def add_method(cls, fun):\n",
    "    setattr(cls, fun.__name__, fun)\n",
    "    return fun\n",
    "\n",
    "  def __init__(self, text_processor, units):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.text_processor = text_processor\n",
    "    self.vocab_size = text_processor.vocabulary_size()\n",
    "    self.word_to_id = tf.keras.layers.StringLookup(\n",
    "        vocabulary=text_processor.get_vocabulary(),\n",
    "        mask_token='', oov_token='[UNK]')\n",
    "    self.id_to_word = tf.keras.layers.StringLookup(\n",
    "        vocabulary=text_processor.get_vocabulary(),\n",
    "        mask_token='', oov_token='[UNK]',\n",
    "        invert=True)\n",
    "    self.start_token = self.word_to_id('[START]')\n",
    "    self.end_token = self.word_to_id('[END]')\n",
    "\n",
    "    self.units = units\n",
    "\n",
    "\n",
    "    # 1. The embedding layer converts token IDs to vectors\n",
    "    self.embedding = tf.keras.layers.Embedding(self.vocab_size,\n",
    "                                               units, mask_zero=True)\n",
    "\n",
    "    # 2. The RNN keeps track of what's been generated so far.\n",
    "    self.rnn = tf.keras.layers.GRU(units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    # 3. The RNN output will be the query for the attention layer.\n",
    "    self.attention = CrossAttention(units)\n",
    "\n",
    "    # 4. This fully connected layer produces the logits for each\n",
    "    # output token.\n",
    "    self.output_layer = tf.keras.layers.Dense(self.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sd8-nRNzFR8x"
   },
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPnaw583CpnY"
   },
   "source": [
    "Next, the `call` method, takes 3 arguments:\n",
    "\n",
    "* `inputs` -  a `context, x` pair where:\n",
    "  * `context` - is the context from the encoder's output.\n",
    "  * `x` - is the target sequence input.\n",
    "* `state` - Optional, the previous `state` output from the decoder (the internal state of the decoder's RNN). Pass the state from a previous run to continue generating text where you left off.\n",
    "* `return_state` - [Default: False] - Set this to `True` to return the RNN state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "PJOi5btHAPNK"
   },
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "def call(self,\n",
    "         context, x,\n",
    "         state=None,\n",
    "         return_state=False):  \n",
    "  shape_checker = ShapeChecker()\n",
    "  shape_checker(x, 'batch t')\n",
    "  shape_checker(context, 'batch s units')\n",
    "\n",
    "  # 1. Lookup the embeddings\n",
    "  x = self.embedding(x)\n",
    "  shape_checker(x, 'batch t units')\n",
    "\n",
    "  # 2. Process the target sequence.\n",
    "  x, state = self.rnn(x, initial_state=state)\n",
    "  shape_checker(x, 'batch t units')\n",
    "\n",
    "  # 3. Use the RNN output as the query for the attention over the context.\n",
    "  x = self.attention(x, context)\n",
    "  self.last_attention_weights = self.attention.last_attention_weights\n",
    "  shape_checker(x, 'batch t units')\n",
    "  shape_checker(self.last_attention_weights, 'batch t s')\n",
    "\n",
    "  # Step 4. Generate logit predictions for the next token.\n",
    "  logits = self.output_layer(x)\n",
    "  shape_checker(logits, 'batch t target_vocab_size')\n",
    "\n",
    "  if return_state:\n",
    "    return logits, state\n",
    "  else:\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1-mLAcUEXpK"
   },
   "source": [
    "That will be sufficient for training. Create an instance of the decoder to test out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "4ZUMbYXIEVeA"
   },
   "outputs": [],
   "source": [
    "decoder = Decoder(target_text_processor, UNITS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFWaI4wqzt4t"
   },
   "source": [
    "In training you'll use the decoder like this:\n",
    "\n",
    "Given the context and target tokens, for each target token it predicts the next target token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "5YM-lD7bzx18"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielchan/miniconda3/lib/python3.11/site-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'cross_attention_7' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/Users/danielchan/miniconda3/lib/python3.11/site-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'decoder_4', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder output shape: (batch, s, units) (64, 196, 256)\n",
      "input target tokens shape: (batch, t) (64, 294)\n",
      "logits shape shape: (batch, target_vocabulary_size) (64, 294, 172)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielchan/miniconda3/lib/python3.11/site-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'decoder_4' (of type Decoder) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "logits = decoder(ex_context, ex_tar_in)\n",
    "\n",
    "print(f'encoder output shape: (batch, s, units) {ex_context.shape}')\n",
    "print(f'input target tokens shape: (batch, t) {ex_tar_in.shape}')\n",
    "print(f'logits shape shape: (batch, target_vocabulary_size) {logits.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhS_tbk7VQkX"
   },
   "source": [
    "#### Inference\n",
    "\n",
    "To use it for inference you'll need a couple more methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "SPm12cnIVRQr"
   },
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "def get_initial_state(self, context):\n",
    "  batch_size = tf.shape(context)[0]\n",
    "  start_tokens = tf.fill([batch_size, 1], self.start_token)\n",
    "  done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
    "  embedded = self.embedding(start_tokens)\n",
    "  return start_tokens, done, self.rnn.get_initial_state(batch_size)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "id": "TzeOhpBvVS5L"
   },
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "def tokens_to_text(self, tokens):\n",
    "  words = self.id_to_word(tokens)\n",
    "  result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
    "  result = tf.strings.regex_replace(result, '^ *\\[START\\] *', '')\n",
    "  result = tf.strings.regex_replace(result, ' *\\[END\\] *$', '')\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "v6ildnz_V1MA"
   },
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "def get_next_token(self, context, next_token, done, state, temperature = 0.0):\n",
    "  logits, state = self(\n",
    "    context, next_token,\n",
    "    state = state,\n",
    "    return_state=True) \n",
    "  \n",
    "  if temperature == 0.0:\n",
    "    next_token = tf.argmax(logits, axis=-1)\n",
    "  else:\n",
    "    logits = logits[:, -1, :]/temperature\n",
    "    next_token = tf.random.categorical(logits, num_samples=1)\n",
    "\n",
    "  # If a sequence produces an `end_token`, set it `done`\n",
    "  done = done | (next_token == self.end_token)\n",
    "  # Once a sequence is done it only produces 0-padding.\n",
    "  next_token = tf.where(done, tf.constant(0, dtype=tf.int64), next_token)\n",
    "  \n",
    "  return next_token, done, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WiXLrVs-FTE"
   },
   "source": [
    "With those extra functions, you can write a generation loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "SuehagxL-JBZ"
   },
   "outputs": [],
   "source": [
    "# Setup the loop variables.\n",
    "next_token, done, state = decoder.get_initial_state(ex_context)\n",
    "tokens = []\n",
    "\n",
    "for n in range(10):\n",
    "  # Run one step.\n",
    "  next_token, done, state = decoder.get_next_token(\n",
    "      ex_context, next_token, done, state, temperature=1.0)\n",
    "  # Add the token to the output.\n",
    "  tokens.append(next_token)\n",
    "\n",
    "# Stack all the tokens together.\n",
    "tokens = tf.concat(tokens, axis=-1) # (batch, t)\n",
    "\n",
    "# Convert the tokens back to a a string\n",
    "result = decoder.tokens_to_text(tokens)\n",
    "# result[:3].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ALTdqCMLGSY"
   },
   "source": [
    "Since the model's untrained, it outputs items from the vocabulary almost uniformly at random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6xyru86m914"
   },
   "source": [
    "## The model\n",
    "\n",
    "Now that you have all the model components, combine them to build the model for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "WWIyuy71TkJT"
   },
   "outputs": [],
   "source": [
    "class Translator(tf.keras.Model):\n",
    "  @classmethod\n",
    "  def add_method(cls, fun):\n",
    "    setattr(cls, fun.__name__, fun)\n",
    "    return fun\n",
    "\n",
    "  def __init__(self, units,\n",
    "               context_text_processor,\n",
    "               target_text_processor):\n",
    "    super().__init__()\n",
    "    # Build the encoder and decoder\n",
    "    encoder = Encoder(context_text_processor, units)\n",
    "    decoder = Decoder(target_text_processor, units)\n",
    "\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "\n",
    "  def call(self, inputs):\n",
    "    context, x = inputs\n",
    "    context = self.encoder(context)\n",
    "    logits = self.decoder(context, x)\n",
    "\n",
    "    #TODO(b/250038731): remove this\n",
    "    try:\n",
    "      # Delete the keras mask, so keras doesn't scale the loss+accuracy. \n",
    "      del logits._keras_mask\n",
    "    except AttributeError:\n",
    "      pass\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rPi0FkS2iA5"
   },
   "source": [
    "During training the model will be used like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "8vhjTh84K6Mg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielchan/miniconda3/lib/python3.11/site-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'cross_attention_8' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/Users/danielchan/miniconda3/lib/python3.11/site-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'decoder_5', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/Users/danielchan/miniconda3/lib/python3.11/site-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'decoder_5' (of type Decoder) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context tokens, shape: (batch, s, units) (64, 196)\n",
      "Target tokens, shape: (batch, t) (64, 294)\n",
      "logits, shape: (batch, t, target_vocabulary_size) (64, 294, 172)\n"
     ]
    }
   ],
   "source": [
    "model = Translator(UNITS, context_text_processor, target_text_processor)\n",
    "\n",
    "logits = model((ex_context_tok, ex_tar_in))\n",
    "\n",
    "print(f'Context tokens, shape: (batch, s, units) {ex_context_tok.shape}')\n",
    "print(f'Target tokens, shape: (batch, t) {ex_tar_in.shape}')\n",
    "print(f'logits, shape: (batch, t, target_vocabulary_size) {logits.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ch_71VbIRfK"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FmzjGmprVmE"
   },
   "source": [
    "For training, you'll want to implement your own masked loss and accuracy functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "WmTHr5iV3jFr"
   },
   "outputs": [],
   "source": [
    "def masked_loss(y_true, y_pred):\n",
    "    # Calculate the loss for each item in the batch.\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')\n",
    "    loss = loss_fn(y_true, y_pred)\n",
    "\n",
    "    # Mask off the losses on padding.\n",
    "    mask = tf.cast(y_true != 0, loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    # Return the total.\n",
    "    return tf.reduce_sum(loss)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "nRB1CTmQWOIL"
   },
   "outputs": [],
   "source": [
    "def masked_acc(y_true, y_pred):\n",
    "    # Calculate the loss for each item in the batch.\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "    y_pred = tf.cast(y_pred, y_true.dtype)\n",
    "    \n",
    "    match = tf.cast(y_true == y_pred, tf.float32)\n",
    "    mask = tf.cast(y_true != 0, tf.float32)\n",
    "    \n",
    "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f32GuAhw2nXm"
   },
   "source": [
    "Configure the model for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "9g0DRRvm3l9X"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=masked_loss, \n",
    "              metrics=[masked_acc, masked_loss])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DWLI3pssjnx"
   },
   "source": [
    "The model is randomly initialized, and should give roughly uniform output probabilities. So it's easy to predict what the initial values of the metrics should be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "BuP3_LFENMJG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'expected_loss': 5.1474943, 'expected_acc': 0.005813953488372093}"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 1.0 * target_text_processor.vocabulary_size()\n",
    "\n",
    "{\"expected_loss\": tf.math.log(vocab_size).numpy(),\n",
    " \"expected_acc\": 1/vocab_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frVba49Usd0Z"
   },
   "source": [
    "That should roughly match the values returned by running a few steps of evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "8rJITfxEsHKR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 554ms/step - loss: 5.2121 - masked_acc: 0.0068 - masked_loss: 5.2121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 5.211590766906738,\n",
       " 'masked_acc': 0.006894285790622234,\n",
       " 'masked_loss': 5.211590766906738}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(val_ds, steps=20, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "id": "BQd_esVVoSf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m100/100\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 1s/step - loss: 3.8728 - masked_acc: 0.1537 - masked_loss: 3.8728 - val_loss: 3.4641 - val_masked_acc: 0.1886 - val_masked_loss: 3.4641\n",
      "Epoch 2/100\n",
      "\u001b[1m 37/100\u001b[0m \u001b[32mâââââââ\u001b[0m\u001b[37mâââââââââââââ\u001b[0m \u001b[1m1:03\u001b[0m 1s/step - loss: 3.4537 - masked_acc: 0.1891 - masked_loss: 3.4537"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[161], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:318\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    317\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 318\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    320\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1515\u001b[0m   )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds.repeat(), \n",
    "    epochs=100,\n",
    "    steps_per_epoch = 100,\n",
    "    validation_data=val_ds,\n",
    "    validation_steps = 20,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38rLdlmtQHCm"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.ylim([0, max(plt.ylim())])\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('CE/token')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KkhXRASNG80_"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['masked_acc'], label='accuracy')\n",
    "plt.plot(history.history['val_masked_acc'], label='val_accuracy')\n",
    "plt.ylim([0, max(plt.ylim())])\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('CE/token')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mU3Ce8M6I3rz"
   },
   "source": [
    "### Translate\n",
    "\n",
    "Now that the model is trained, implement a function to execute the full `text => text` translation. This code is basically identical to the [inference example](#inference) in the [decoder section](#the_decoder), but this also captures the attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "mmgYPCVgEwp_"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "@Translator.add_method\n",
    "def translate(self,\n",
    "              texts, *,\n",
    "              max_length=50,\n",
    "              temperature=0.0):\n",
    "  # Process the input texts\n",
    "  context = self.encoder.convert_input(texts)\n",
    "  batch_size = tf.shape(texts)[0]\n",
    "\n",
    "  # Setup the loop inputs\n",
    "  tokens = []\n",
    "  attention_weights = []\n",
    "  next_token, done, state = self.decoder.get_initial_state(context)\n",
    "\n",
    "  for _ in range(max_length):\n",
    "    # Generate the next token\n",
    "    next_token, done, state = self.decoder.get_next_token(\n",
    "        context, next_token, done,  state, temperature)\n",
    "        \n",
    "    # Collect the generated tokens\n",
    "    tokens.append(next_token)\n",
    "    attention_weights.append(self.decoder.last_attention_weights)\n",
    "    \n",
    "    if tf.executing_eagerly() and tf.reduce_all(done):\n",
    "      break\n",
    "\n",
    "  # Stack the lists of tokens and attention weights.\n",
    "  tokens = tf.concat(tokens, axis=-1)   # t*[(batch 1)] -> (batch, t)\n",
    "  self.last_attention_weights = tf.concat(attention_weights, axis=1)  # t*[(batch 1 s)] -> (batch, t s)\n",
    "\n",
    "  result = self.decoder.tokens_to_text(tokens)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4XufRntbbva"
   },
   "source": [
    "Here are the two helper methods, used above, to convert tokens to text, and to get the next token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "E5hqvbR5FUCD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielchan/miniconda3/lib/python3.11/site-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/Users/danielchan/miniconda3/lib/python3.11/site-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/Users/danielchan/miniconda3/lib/python3.11/site-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/Users/danielchan/miniconda3/lib/python3.11/site-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'cross_attention_5' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/Users/danielchan/miniconda3/lib/python3.11/site-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'decoder_3' (of type Decoder) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ã® ã¨ ã³ ã§ ã ã '"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.translate(['ã®äººã§ãã']) # Are you still home\n",
    "result[0].numpy().decode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQ1iU63cVgfs"
   },
   "source": [
    "Use that to generate the attention plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "s5hQWlbN3jGF"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "@Translator.add_method\n",
    "def plot_attention(self, text, **kwargs):\n",
    "  assert isinstance(text, str)\n",
    "  output = self.translate([text], **kwargs)\n",
    "  output = output[0].numpy().decode()\n",
    "\n",
    "  attention = self.last_attention_weights[0]\n",
    "\n",
    "  context = tf_lower_and_split_punct(text)\n",
    "  context = context.numpy().decode().split()\n",
    "\n",
    "  output = tf_lower_and_split_punct(output)\n",
    "  output = output.numpy().decode().split()[1:]\n",
    "\n",
    "  fig = plt.figure(figsize=(10, 10))\n",
    "  ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "  ax.matshow(attention, cmap='viridis', vmin=0.0)\n",
    "\n",
    "  fontdict = {'fontsize': 14}\n",
    "\n",
    "  ax.set_xticklabels([''] + context, fontdict=fontdict, rotation=90)\n",
    "  ax.set_yticklabels([''] + output, fontdict=fontdict)\n",
    "\n",
    "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "  ax.set_xlabel('Input text')\n",
    "  ax.set_ylabel('Output text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "rrGawQv2eiA4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielchan/miniconda3/lib/python3.11/site-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/Users/danielchan/miniconda3/lib/python3.11/site-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/Users/danielchan/miniconda3/lib/python3.11/site-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/Users/danielchan/miniconda3/lib/python3.11/site-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'cross_attention_5' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/Users/danielchan/miniconda3/lib/python3.11/site-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'decoder_3' (of type Decoder) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/var/folders/pq/sl3k6ld12gqbtw479x2nv_r00000gn/T/ipykernel_67527/3355722706.py:23: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels([''] + context, fontdict=fontdict, rotation=90)\n",
      "/var/folders/pq/sl3k6ld12gqbtw479x2nv_r00000gn/T/ipykernel_67527/3355722706.py:24: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_yticklabels([''] + output, fontdict=fontdict)\n",
      "/Users/danielchan/miniconda3/lib/python3.11/site-packages/IPython/core/events.py:93: UserWarning: Glyph 35504 (\\N{CJK UNIFIED IDEOGRAPH-8AB0}) missing from font(s) DejaVu Sans.\n",
      "  func(*args, **kwargs)\n",
      "/Users/danielchan/miniconda3/lib/python3.11/site-packages/IPython/core/events.py:93: UserWarning: Glyph 12391 (\\N{HIRAGANA LETTER DE}) missing from font(s) DejaVu Sans.\n",
      "  func(*args, **kwargs)\n",
      "/Users/danielchan/miniconda3/lib/python3.11/site-packages/IPython/core/events.py:93: UserWarning: Glyph 12377 (\\N{HIRAGANA LETTER SU}) missing from font(s) DejaVu Sans.\n",
      "  func(*args, **kwargs)\n",
      "/Users/danielchan/miniconda3/lib/python3.11/site-packages/IPython/core/events.py:93: UserWarning: Glyph 12363 (\\N{HIRAGANA LETTER KA}) missing from font(s) DejaVu Sans.\n",
      "  func(*args, **kwargs)\n",
      "/Users/danielchan/miniconda3/lib/python3.11/site-packages/IPython/core/events.py:93: UserWarning: Glyph 12384 (\\N{HIRAGANA LETTER DA}) missing from font(s) DejaVu Sans.\n",
      "  func(*args, **kwargs)\n",
      "/Users/danielchan/miniconda3/lib/python3.11/site-packages/IPython/core/events.py:93: UserWarning: Glyph 12428 (\\N{HIRAGANA LETTER RE}) missing from font(s) DejaVu Sans.\n",
      "  func(*args, **kwargs)\n",
      "/Users/danielchan/miniconda3/lib/python3.11/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 35504 (\\N{CJK UNIFIED IDEOGRAPH-8AB0}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/danielchan/miniconda3/lib/python3.11/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 12391 (\\N{HIRAGANA LETTER DE}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/danielchan/miniconda3/lib/python3.11/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 12377 (\\N{HIRAGANA LETTER SU}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/danielchan/miniconda3/lib/python3.11/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 12363 (\\N{HIRAGANA LETTER KA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/danielchan/miniconda3/lib/python3.11/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 12384 (\\N{HIRAGANA LETTER DA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/danielchan/miniconda3/lib/python3.11/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 12428 (\\N{HIRAGANA LETTER RE}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.plot_attention('èª°ã§ãã') # Are you still home"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHBdOf9duumm"
   },
   "source": [
    "Translate a few more sentences and plot them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "flT0VlQZK11s"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# This is my life.\n",
    "model.plot_attention('Esta es mi vida.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-fPYP_9K8xa"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    " # Try to find out.'\n",
    "model.plot_attention('Tratar de descubrir.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rA3xI3NzrRJt"
   },
   "source": [
    "The short sentences often work well, but if the input is too long the model literally loses focus and stops providing reasonable predictions. There are two main reasons for this:\n",
    "\n",
    "1. The model was trained with teacher-forcing feeding the correct token at each step, regardless of the model's predictions. The model could be made more robust if it were sometimes fed its own predictions.\n",
    "2. The model only has access to its previous output through the RNN state. If the RNN state looses track of where it was in the context sequence there's no way for the model to recover. [Transformers](transformer.ipynb) improve on this by letting the decoder look at what it has output so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vtz6QBoGWqT2"
   },
   "source": [
    "The raw data is sorted by length, so try translating the longest sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-FUHFLEvSMbG"
   },
   "outputs": [],
   "source": [
    "long_text = context_raw[-1]\n",
    "\n",
    "import textwrap\n",
    "print('Expected output:\\n', '\\n'.join(textwrap.wrap(target_raw[-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lDa_8NaN_RUy"
   },
   "outputs": [],
   "source": [
    "model.plot_attention(long_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PToqG3GiIUPM"
   },
   "source": [
    "The `translate` function works on batches, so if you have multiple texts to translate you can pass them all at once, which is much more efficient than translating them one at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1-FLCjBVEMXL"
   },
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    'Hace mucho frio aqui.', # \"It's really cold here.\"\n",
    "    'Esta es mi vida.', # \"This is my life.\"\n",
    "    'Su cuarto es un desastre.' # \"His room is a mess\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sT68i4jYEQ7q"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for t in inputs:\n",
    "  print(model.translate([t])[0].numpy().decode())\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hd2rgyHwVVrv"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "result = model.translate(inputs)\n",
    "\n",
    "print(result[0].numpy().decode())\n",
    "print(result[1].numpy().decode())\n",
    "print(result[2].numpy().decode())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvhMqIw26Bwd"
   },
   "source": [
    "So overall this text generation function mostly gets the job done, but so you've only used it here in python with eager execution. Let's try to export it next:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4POAuUgLxLv"
   },
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-6cFyqeUPQm"
   },
   "source": [
    "If you want to export this model you'll need to wrap the `translate` method in a `tf.function`. That implementation will get the job done:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fNhGwQaVKIAy"
   },
   "outputs": [],
   "source": [
    "class Export(tf.Module):\n",
    "  def __init__(self, model):\n",
    "    self.model = model\n",
    "\n",
    "  @tf.function(input_signature=[tf.TensorSpec(dtype=tf.string, shape=[None])])\n",
    "  def translate(self, inputs):\n",
    "    return self.model.translate(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Tjqs9FzNwW5"
   },
   "outputs": [],
   "source": [
    "export = Export(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fkccvHDvXCa8"
   },
   "source": [
    "Run the `tf.function` once to compile it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_NzrixLvVBjQ"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "_ = export.translate(tf.constant(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "USJdu00tVFbd"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "result = export.translate(tf.constant(inputs))\n",
    "\n",
    "print(result[0].numpy().decode())\n",
    "print(result[1].numpy().decode())\n",
    "print(result[2].numpy().decode())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NP2dNtEXJPEL"
   },
   "source": [
    "Now that the function has been traced it can be exported using `saved_model.save`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OyvxT5V0_X5B"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tf.saved_model.save(export, 'translator',\n",
    "                    signatures={'serving_default': export.translate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-I0j3i3ekOba"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "reloaded = tf.saved_model.load('translator')\n",
    "_ = reloaded.translate(tf.constant(inputs)) #warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GXZF__FZXJCm"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "result = reloaded.translate(tf.constant(inputs))\n",
    "\n",
    "print(result[0].numpy().decode())\n",
    "print(result[1].numpy().decode())\n",
    "print(result[2].numpy().decode())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pgg3P757O5rw"
   },
   "source": [
    "#### [Optional] Use a dynamic loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3230LfyRIJQV"
   },
   "source": [
    "It's worth noting that this initial implementation is not optimal. It uses a python loop:\n",
    "\n",
    "```\n",
    "for _ in range(max_length):\n",
    "  ...\n",
    "  if tf.executing_eagerly() and tf.reduce_all(done):\n",
    "    break\n",
    "```\n",
    "\n",
    "The python loop is relatively simple but when `tf.function` converts this to a graph, it **statically unrolls** that loop. Unrolling the loop has two disadvantages:\n",
    "\n",
    "1. It makes `max_length` copies of the loop body. So the generated graphs take longer to build, save and load.\n",
    "1. You have to choose a fixed value for the `max_length`. \n",
    "1. You can't `break` from a statically unrolled loop. The `tf.function`\n",
    "  version will run the full `max_length` iterations on every call.\n",
    "  That's why the `break` only works with eager execution. This is\n",
    "  still marginally faster than eager execution, but not as fast as it could be.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPRJp4TRJx_n"
   },
   "source": [
    "To fix these shortcomings, the `translate_dynamic` method, below, uses a tensorflow loop:\n",
    "\n",
    "```\n",
    "for t in tf.range(max_length):\n",
    "  ...\n",
    "  if tf.reduce_all(done):\n",
    "      break\n",
    "```\n",
    "\n",
    "It looks like a python loop, but when you use a tensor as the input to a `for` loop (or the condition of a `while` loop) `tf.function` converts it to a dynamic loop using operations like `tf.while_loop`. \n",
    "\n",
    "There's no need for a `max_length` here it's just in case the model gets stuck generating a loop like: `the united states of the united states of the united states...`.\n",
    "\n",
    "On the down side, to accumulate tokens from this dynamic loop you can't just append them to a python `list`, you need to use a `tf.TensorArray`:\n",
    "\n",
    "```\n",
    "tokens = tf.TensorArray(tf.int64, size=1, dynamic_size=True)\n",
    "...\n",
    "for t in tf.range(max_length):\n",
    "    ...\n",
    "    tokens = tokens.write(t, next_token) # next_token shape is (batch, 1)\n",
    "  ...\n",
    "  tokens = tokens.stack()\n",
    "  tokens = einops.rearrange(tokens, 't batch 1 -> batch t')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTmISp4SRo5U"
   },
   "source": [
    "This version of the code can be quite a bit more efficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EbQpyYs13jF_"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "@Translator.add_method\n",
    "def translate(self,\n",
    "              texts,\n",
    "              *,\n",
    "              max_length=500,\n",
    "              temperature=tf.constant(0.0)):\n",
    "  shape_checker = ShapeChecker()\n",
    "  context = self.encoder.convert_input(texts)\n",
    "  batch_size = tf.shape(context)[0]\n",
    "  shape_checker(context, 'batch s units')\n",
    "\n",
    "  next_token, done, state = self.decoder.get_initial_state(context)\n",
    "\n",
    "  # initialize the accumulator\n",
    "  tokens = tf.TensorArray(tf.int64, size=1, dynamic_size=True)\n",
    "\n",
    "  for t in tf.range(max_length):\n",
    "    # Generate the next token\n",
    "    next_token, done, state = self.decoder.get_next_token(\n",
    "        context, next_token, done, state, temperature)\n",
    "    shape_checker(next_token, 'batch t1')\n",
    "\n",
    "    # Collect the generated tokens\n",
    "    tokens = tokens.write(t, next_token)\n",
    "\n",
    "    # if all the sequences are done, break\n",
    "    if tf.reduce_all(done):\n",
    "      break\n",
    "\n",
    "  # Convert the list of generated token ids to a list of strings.\n",
    "  tokens = tokens.stack()\n",
    "  shape_checker(tokens, 't batch t1')\n",
    "  tokens = einops.rearrange(tokens, 't batch 1 -> batch t')\n",
    "  shape_checker(tokens, 'batch t')\n",
    "\n",
    "  text = self.decoder.tokens_to_text(tokens)\n",
    "  shape_checker(text, 'batch')\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJ_NznOgZTxC"
   },
   "source": [
    "With eager execution this implementation performs on par with the original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JRh66y-YYeBw"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "result = model.translate(inputs)\n",
    "\n",
    "print(result[0].numpy().decode())\n",
    "print(result[1].numpy().decode())\n",
    "print(result[2].numpy().decode())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6B8W4_MZdX0"
   },
   "source": [
    "But when you wrap it in a `tf.function` you'll notice two differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EQlrhWWrUhgT"
   },
   "outputs": [],
   "source": [
    "class Export(tf.Module):\n",
    "  def __init__(self, model):\n",
    "    self.model = model\n",
    "\n",
    "  @tf.function(input_signature=[tf.TensorSpec(dtype=tf.string, shape=[None])])\n",
    "  def translate(self, inputs):\n",
    "    return self.model.translate(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pH8yyGHvUmti"
   },
   "outputs": [],
   "source": [
    "export = Export(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnOJvIsvUwBL"
   },
   "source": [
    "First, it's much quicker to trace, since it only creates one copy of the loop body:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_CaEbHkwEa1S"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "_ = export.translate(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ABEwtKIZ6eE"
   },
   "source": [
    "The `tf.function` is much faster than running with eager execution, and on small inputs it's often several times faster than the unrolled version, because it can break out of the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5VdCLxPYrpz"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "result = export.translate(inputs)\n",
    "\n",
    "print(result[0].numpy().decode())\n",
    "print(result[1].numpy().decode())\n",
    "print(result[2].numpy().decode())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DDmofICJdx0"
   },
   "source": [
    "So save this version as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eCg7kRq6FVl3"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tf.saved_model.save(export, 'dynamic_translator',\n",
    "                    signatures={'serving_default': export.translate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zrpzxL2vFVl3"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "reloaded = tf.saved_model.load('dynamic_translator')\n",
    "_ = reloaded.translate(tf.constant(inputs)) #warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5TjSwrCEFVl3"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "result = reloaded.translate(tf.constant(inputs))\n",
    "\n",
    "print(result[0].numpy().decode())\n",
    "print(result[1].numpy().decode())\n",
    "print(result[2].numpy().decode())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"donde esta la biblioteca\"]\n",
    "input = tf.constant(texts)\n",
    "result = reloaded.translate(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0].numpy().decode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTe5P5ioMJwN"
   },
   "source": [
    "## Next steps\n",
    "\n",
    "* [Download a different dataset](http://www.manythings.org/anki/) to experiment with translations, for example, English to German, or English to French.\n",
    "* Experiment with training on a larger dataset, or using more epochs.\n",
    "* Try the [transformer tutorial](transformer.ipynb) which implements a similar translation task but uses transformer layers instead of RNNs. This version also uses a `text.BertTokenizer` to implement word-piece tokenization.\n",
    "* Visit the [`tensorflow_addons.seq2seq` tutorial](https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt), which demonstrates a higher-level functionality for implementing this sort of sequence-to-sequence model, such as `seq2seq.BeamSearchDecoder`."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "last_runtime": {
    "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
    "kind": "private"
   },
   "name": "nmt_with_attention.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
